""" 
 äº‹ä»¶æ€»çº¿å®ç°ï¼ˆè½»é‡çº§ SDKï¼‰ 
 - å¯¹å¤–æä¾›åŒæ­¥æ¥å£ï¼ˆå¦‚ publish_task_eventï¼‰ 
 - å†…éƒ¨é€šè¿‡åå°çº¿ç¨‹ + asyncio loop å¼‚æ­¥å‘é€ HTTP è¯·æ±‚ 
 - æ”¯æŒç¼“å†²ã€é‡è¯•ã€è§£è€¦ 
 - é˜Ÿåˆ—å¯æœªæ¥æ›¿æ¢ä¸º Redisï¼ˆåªéœ€æ”¹ queue å®ç°ï¼‰ 
 """

from typing import Dict, Any, Optional, List
import logging
import httpx
import uuid
import os
from datetime import datetime
import asyncio
import threading
import queue
import time
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, date
from decimal import Decimal
import numpy as np
from pydantic import BaseModel

# å¯¼å…¥ä¿¡å·çŠ¶æ€æšä¸¾
from common.signal.signal_status import SignalStatus

# ä»ç¯å¢ƒå˜é‡è·å– events æœåŠ¡åœ°å€
EVENTS_SERVICE_URL = os.getenv('EVENTS_SERVICE_URL', 'http://localhost:8000') 


class EventType(Enum): 
    TASK_EVENT = "task_event" 
    SPLIT_REQUEST = "split_request" 


@dataclass 
class QueuedEvent: 
    event_type: EventType 
    payload: Dict[str, Any]  # åŒ…å«æ‰€æœ‰å¿…è¦å‚æ•° 
    retry_count: int = 0 
    max_retries: int = 3 


class EventPublisher: 
    """ 
    è½»é‡çº§äº‹ä»¶å‘å¸ƒ SDK 
    - åŒæ­¥æ–¹æ³•ä»…å…¥é˜Ÿï¼Œéé˜»å¡ 
    - åå°çº¿ç¨‹å¼‚æ­¥æ¶ˆè´¹å¹¶å‘é€ HTTP è¯·æ±‚ 
    """

    def __init__( 
        self, 
        lifecycle_base_url: str = "http://localhost:8004", 
        logger: Optional[logging.Logger] = None, 
        max_queue_size: int = 10_000, 
        shutdown_timeout: float = 5.0, 
    ): 
        """åˆå§‹åŒ–äº‹ä»¶æ€»çº¿""" 
        self.base_url = lifecycle_base_url.rstrip("/") 
        self.log = logger or logging.getLogger(f"{__name__}.{self.__class__.__name__}") 
        self.shutdown_timeout = shutdown_timeout 

        # å†…å­˜é˜Ÿåˆ—ï¼ˆæœªæ¥å¯æ›¿æ¢ä¸º RedisQueueï¼‰ 
        self._queue: queue.Queue[QueuedEvent] = queue.Queue(maxsize=max_queue_size) 

        # åå°çº¿ç¨‹å’Œ asyncio loop 
        self._loop: Optional[asyncio.AbstractEventLoop] = None 
        self._thread: Optional[threading.Thread] = None 
        self._running = threading.Event() 
        self._shutdown_complete = threading.Event() 

        # å¯åŠ¨åå°æ¶ˆè´¹è€…çº¿ç¨‹ 
        self._start_background_worker() 

        self.log.info(f"EventPublisher initialized with base URL: {lifecycle_base_url}") 

    def _start_background_worker(self): 
        """å¯åŠ¨åå°çº¿ç¨‹è¿è¡Œ asyncio loop""" 
        def run_loop(): 
            self._loop = asyncio.new_event_loop() 
            asyncio.set_event_loop(self._loop) 
            self._running.set() 
            try: 
                self._loop.run_until_complete(self._consume_queue()) 
            finally: 
                self._loop.close() 
                self._shutdown_complete.set() 

        self._thread = threading.Thread(target=run_loop, daemon=True, name="EventPublisherWorker") 
        self._thread.start() 
        self._running.wait()  # ç­‰å¾… loop å°±ç»ª 

    async def _consume_queue(self): 
        """å¼‚æ­¥æ¶ˆè´¹é˜Ÿåˆ—ä¸­çš„äº‹ä»¶""" 
        client = httpx.AsyncClient(timeout=10.0) 
        try: 
            while True: 
                try: 
                    # ä½¿ç”¨ asyncio çš„æ–¹å¼ä» queue ä¸­å–ï¼ˆéœ€åŒ…è£…ï¼‰ 
                    item = await asyncio.get_event_loop().run_in_executor(None, self._queue.get, True, 1.0) 
                    if item is None:  # ç”¨äºè§¦å‘é€€å‡º 
                        break 
                except queue.Empty: 
                    continue 

                try: 
                    if item.event_type == EventType.TASK_EVENT: 
                        await self._send_event_request_internal(client, item.payload) 
                    elif item.event_type == EventType.SPLIT_REQUEST: 
                        await self._send_split_request_internal(client, item.payload) 
                    self._queue.task_done() 
                except Exception as e: 
                    self.log.error(f"Failed to process queued event: {e}", exc_info=True) 
                    # é‡è¯•æœºåˆ¶ 
                    if item.retry_count < item.max_retries: 
                        item.retry_count += 1 
                        delay = (2 ** item.retry_count) * 0.5  # æŒ‡æ•°é€€é¿ 
                        await asyncio.sleep(delay) 
                        self._queue.put_nowait(item) 
                    else: 
                        self.log.error(f"Event dropped after {item.max_retries} retries: {item.payload}") 
                        self._queue.task_done() 
        except asyncio.CancelledError: 
            pass 
        finally: 
            await client.aclose() 

    # ======================== 
    # å†…éƒ¨ async æ–¹æ³•ï¼ˆä»…ä¾›åå°ä½¿ç”¨ï¼‰ 
    # ======================== 

    async def _send_split_request_internal(self, client: httpx.AsyncClient, payload: Dict): 
        """
        ä¸“ç”¨é€šé“ï¼šå‘é€è£‚å˜è¯·æ±‚
        """
        
        url = f"{self.base_url}/api/v1/traces/{payload['trace_id']}/split" 
        try: 
            # è°ƒæ•´è¯·æ±‚ä½“å­—æ®µï¼Œå°† snapshot æ”¹ä¸º reasoning_snapshot
            split_data = payload["data"].copy()
            if "snapshot" in split_data:
                split_data["reasoning_snapshot"] = split_data.pop("snapshot")
            
            resp = await client.post(url, json=split_data) 
            if resp.status_code >= 400: 
                self.log.error(f"Split task failed: {resp.status_code} - {resp.text}") 
            else: 
                self.log.info(f"Task split successfully: {len(split_data['subtasks_meta'])} subtasks created.") 
        except Exception as e: 
            self.log.error(f"Failed to send split request: {str(e)}") 
            raise 

    async def _send_event_request_internal(self, client: httpx.AsyncClient, payload: Dict): 
        """
        é€šç”¨é€šé“ï¼šå‘é€çŠ¶æ€äº‹ä»¶
        """
        
        url = f"{self.base_url}/api/v1/traces/events" 
        try: 
            serializable_payload = self.serialize_payload(payload)
            resp = await client.post(url, json=serializable_payload) 
            if resp.status_code >= 400: 
                self.log.error(f"Event report failed: {resp.status_code} - {resp.text}") 
            else: 
                self.log.debug(f"Lifecycle event sent: {payload.get('event_type')}") 
        except Exception as e: 
            self.log.error(f"Failed to send event: {str(e)}") 
            raise 

    # ======================== 
    # åŒæ­¥å…¥å£ï¼ˆå¯¹å¤– APIï¼‰ 
    # ======================== 

    def publish_task_event( 
        self, 
        task_id: str, 
        event_type: str, 
        trace_id: str, 
        task_path: str, 
        source: str, 
        agent_id: str, 
        data: Optional[Dict[str, Any]] = None, 
        user_id: Optional[str] = None, 
        message_type: Optional[str] = None, 
        enriched_context_snapshot: Optional[Dict[str, Any]] = None, 
        error: Optional[str] = None, 
        name: Optional[str] = None
    ) -> None: 
        """ 
        åŒæ­¥å…¥å£ï¼šéé˜»å¡åœ°å°†äº‹ä»¶åŠ å…¥é˜Ÿåˆ— 
        """ 
        try: 
            safe_data = data or {} 
            
            # ç‰¹æ®Šå¤„ç† TASK_DISPATCHED â†’ èµ° split è·¯ç”± 
            if event_type == "TASK_DISPATCHED": 
                plans = safe_data.get("plans", []) 
                if plans: 
                    subtasks_meta = [self._adapt_plan_to_meta(p) for p in plans] 
                    snapshot = enriched_context_snapshot or { 
                        "reasoning": safe_data.get("message", ""), 
                        "raw_plans": plans 
                    } 
                    split_payload = { 
                        "trace_id": trace_id, 
                        "parent_id": task_id, 
                        "subtasks_meta": subtasks_meta, 
                        "snapshot": snapshot, 
                    } 
                    event = QueuedEvent( 
                        event_type=EventType.SPLIT_REQUEST, 
                        payload={"trace_id": trace_id, "data": split_payload} 
                    ) 
                    self._enqueue(event) 
                    return 

            # æ™®é€šäº‹ä»¶
            safe_data.update({
                "task_path": task_path,
                "message_type": message_type,
                "user_id": user_id,
            })
            
            # è½¬æ¢äº‹ä»¶ç±»å‹ä¸ºlifecycleæœåŠ¡èƒ½ç†è§£çš„ç®€å•ç±»å‹
            lifecycle_event_type = event_type
            if event_type == "TASK_RUNNING":
                lifecycle_event_type = "STARTED"
            elif event_type == "TASK_COMPLETED":
                lifecycle_event_type = "COMPLETED"
            elif event_type == "TASK_FAILED":
                lifecycle_event_type = "FAILED"
            elif event_type == "TASK_PROGRESS":
                lifecycle_event_type = "PROGRESS"
            
            payload = {
                "task_id": task_id,
                "event_type": lifecycle_event_type,
                "trace_id": trace_id,
                "source": source,
                "agent_id": agent_id,
                "data": safe_data,
                "error": error,
                "name": name,
                "enriched_context_snapshot": enriched_context_snapshot,
                "timestamp": datetime.now().timestamp(),
            } 
            event = QueuedEvent(event_type=EventType.TASK_EVENT, payload=payload) 
            self._enqueue(event) 

        except Exception as e: 
            self.log.error(f"Failed to enqueue event: {e}", exc_info=True) 

    def _enqueue(self, event: QueuedEvent): 
        """å®‰å…¨å…¥é˜Ÿï¼Œä¸¢å¼ƒç­–ç•¥ï¼šå¦‚æœé˜Ÿåˆ—æ»¡åˆ™è®°å½•è­¦å‘Šå¹¶ä¸¢å¼ƒ""" 
        try: 
            self._queue.put_nowait(event) 
        except queue.Full: 
            self.log.warning("Event queue is full. Dropping event.") 

    # ======================== 
    # ä¿ç•™åŸæœ‰ async æ¥å£ï¼ˆä¾›å†…éƒ¨æˆ–æµ‹è¯•ä½¿ç”¨ï¼‰ 
    # ======================== 

    # async def publish_task_event( 
    #     self, 
    #     task_id: str, 
    #     event_type: str, 
    #     trace_id: str, 
    #     task_path: str, 
    #     source: str, 
    #     agent_id: str, 
    #     data: Optional[Dict[str, Any]] = None, 
    #     user_id: Optional[str] = None, 
    #     message_type: Optional[str] = None, 
    #     enriched_context_snapshot: Optional[Dict[str, Any]] = None, 
    #     error: Optional[str] = None, 
    # ): 
    #     """ 
    #     æ™ºèƒ½è·¯ç”±çš„ä¸ŠæŠ¥æ–¹æ³• 
        
    #     Args: 
    #         task_id: ä»»åŠ¡ID 
    #         event_type: äº‹ä»¶ç±»å‹ 
    #         trace_id: ç”¨äºè¿½è¸ªæ•´ä¸ªè°ƒç”¨é“¾ 
    #         task_path: ä»»åŠ¡è·¯å¾„ 
    #         source: äº‹ä»¶æº 
    #         agent_id: æ™ºèƒ½ä½“ID 
    #         data: äº‹ä»¶æ•°æ® 
    #         user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰ 
    #         message_type: æ¶ˆæ¯ç±»å‹ï¼ˆå¯é€‰ï¼‰ 
    #         enriched_context_snapshot: å¿«ç…§å…³é”®ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰ 
    #         error: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰ 
    #     """ 
    #     # å¤ç”¨åŒæ­¥é€»è¾‘ç”Ÿæˆ payloadï¼Œä½†ç›´æ¥ await å‘é€ 
    #     safe_data = data or {} 
    #     if event_type == "TASK_DISPATCHED": 
    #         plans = safe_data.get("plans", []) 
    #         if plans: 
    #             subtasks_meta = [self._adapt_plan_to_meta(p) for p in plans] 
    #             snapshot = enriched_context_snapshot or { 
    #                 "reasoning": safe_data.get("message", ""), 
    #                 "raw_plans": plans 
    #             } 
    #             await self._send_split_request(trace_id, task_id, subtasks_meta, snapshot) 
    #             return 

    #     safe_data.update({ 
    #         "task_path": task_path, 
    #         "message_type": message_type, 
    #         "user_id": user_id, 
    #     }) 
    #     payload = { 
    #         "task_id": task_id, 
    #         "event_type": event_type, 
    #         "trace_id": trace_id, 
    #         "source": source, 
    #         "agent_id": agent_id, 
    #         "data": safe_data, 
    #         "error": error, 
    #         "enriched_context_snapshot": enriched_context_snapshot, 
    #         "timestamp": datetime.now().timestamp(), 
    #     } 
    #     await self._send_event_request(payload) 

    # åŸæœ‰æ–¹æ³•ä¿æŒå…¼å®¹ 
    # async def publish( 
    #     self, 
    #     trace_id: str, 
    #     event_type: str, 
    #     source: str, 
    #     data: Dict[str, Any], 
    #     level: str = "INFO" 
    # ): 
    #     """ 
    #     å…¨ç³»ç»Ÿé€šç”¨çš„åŸ‹ç‚¹æ–¹æ³• (æ”¹é€ ä¸ºå¤ç”¨ publish_task_event) 
        
    #     Args: 
    #         trace_id: ç”¨äºè¿½è¸ªæ•´ä¸ªè°ƒç”¨é“¾ (Task ID) 
    #         event_type: äº‹ä»¶ç±»å‹ 
    #         source: äº‹ä»¶æº 
    #         data: äº‹ä»¶æ•°æ® 
    #         level: æ—¥å¿—çº§åˆ« 
    #     """ 
    #     task_id = data.get('task_id', trace_id) 
    #     agent_id = data.get('agent_id', 'unknown') 
    #     task_path = data.get('task_path', source) 
    #     await self.publish_task_event( 
    #         task_id=task_id, 
    #         event_type=event_type, 
    #         trace_id=trace_id, 
    #         task_path=task_path, 
    #         source=source, 
    #         agent_id=agent_id, 
    #         data=data, 
    #         error=data.get('error'), 
    #         enriched_context_snapshot=data.get('snapshot') 
    #     ) 

    def get_signal_status(self, trace_id: str) -> Dict[str, Any]: 
        """ 
        è·å–è·Ÿè¸ªé“¾è·¯çš„å½“å‰ä¿¡å·çŠ¶æ€ 
        
        Args: 
            trace_id: è·Ÿè¸ªé“¾è·¯ID 
            
        Returns: 
            Dict: åŒ…å«ä¿¡å·çŠ¶æ€çš„å“åº”æ•°æ®ï¼Œå…¶ä¸­signalå­—æ®µä¸ºSignalStatusæšä¸¾å€¼ 
            
        Raises: 
            httpx.RequestError: å¦‚æœè¯·æ±‚å¤±è´¥ 
            httpx.HTTPStatusError: å¦‚æœè¿”å›é200çŠ¶æ€ç  
        """ 
        url = f"{self.base_url}/api/v1/traces/{trace_id}/status" 
        with httpx.Client(timeout=10.0) as client: 
            try: 
                resp = client.get(url) 
                resp.raise_for_status() 
                result = resp.json() 
                # æœåŠ¡å™¨è¿”å›çš„æ˜¯ global_signal å­—æ®µï¼Œè€Œä¸æ˜¯ signal å­—æ®µ
                signal_value = result.get('global_signal') 
                if signal_value: 
                    try: 
                        result['signal'] = SignalStatus(signal_value) 
                    except ValueError: 
                        self.log.warning(f"Unknown signal value '{signal_value}' for trace_id {trace_id}, using NORMAL") 
                        result['signal'] = SignalStatus.NORMAL 
                else: 
                    result['signal'] = SignalStatus.NORMAL 
                self.log.info(f"Get signal status successfully for trace_id {trace_id}: {result.get('signal')}") 
                return result 
            except httpx.HTTPStatusError as e: 
                if e.response.status_code == 404: 
                    self.log.warning(f"Signal status not found for trace_id {trace_id}") 
                    raise ValueError(f"Signal status not found for trace_id {trace_id}") from e 
                raise 
            except Exception as e: 
                self.log.error(f"Failed to get signal status: {e}") 
                raise 

    def _adapt_plan_to_meta(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        ã€é€‚é…å™¨ã€‘å°† LLM ç”Ÿæˆçš„ Plan è½¬æ¢ä¸º Lifecycle éœ€è¦çš„ SubtaskMeta
        è¾“å…¥ç¤ºä¾‹:
        {
            "step": 1, "type": "AGENT", "executor": "doc_writer",
            "content": "æ’°å†™æ–‡æ¡£...", "description": "ç”Ÿæˆæ–‡æ¡£"
        }
        """
        return {
            # 1. å¿…é¡»ç”Ÿæˆä¸€ä¸ªæ–°çš„å”¯ä¸€ID
            "id": plan.get("task_id", str(uuid.uuid4())),
            
            # 2. æ˜ å°„ def_id (å…³é”®çº¦å®šï¼šexecutor å­—æ®µå¿…é¡»å¯¹åº”æ•°æ®åº“é‡Œçš„ def_id)
            "def_id": plan.get("executor"),
            
            # 3. æ˜ å°„åç§°
            "name": plan.get("description", f"Step {plan.get('step')}"),
            
            # 4. æ˜ å°„å‚æ•° (æŠŠ content æ”¾å…¥ params)
            "params": {
                "input_instruction": plan.get("content"),
                "step_index": plan.get("step"),
                "task_type": plan.get("type") # AGENT / MCP
            }
        }
    

    
    # ======================== 
    # ç”Ÿå‘½å‘¨æœŸç®¡ç† 
    # ======================== 

    def shutdown(self, timeout: Optional[float] = None): 
        """ä¼˜é›…å…³é—­ï¼šç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæ¯•""" 
        timeout = timeout or self.shutdown_timeout 
        if not self._running.is_set(): 
            return 

        self.log.info("Shutting down EventPublisher...") 
        # åœæ­¢ç”Ÿäº§ 
        self._running.clear() 

        # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæ¯•ï¼ˆæœ€å¤š timeout ç§’ï¼‰ 
        try: 
            self._queue.join() 
        except Exception: 
            pass 

        # å‘é€é€€å‡ºä¿¡å· 
        if self._loop and self._thread.is_alive(): 
            asyncio.run_coroutine_threadsafe(self._stop_consume(), self._loop) 

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ 
        self._shutdown_complete.wait(timeout=timeout) 
        self.log.info("EventPublisher shutdown complete.") 

    async def _stop_consume(self): 
        # æ’å…¥ None ä½œä¸º poison pill 
        await asyncio.get_event_loop().run_in_executor(None, self._queue.put_nowait, None) 

    def __del__(self): 
        if self._running.is_set(): 
            self.shutdown() 

    def serialize_payload(self,obj):
        # å¤„ç†å¸¸è§é JSON ç±»å‹
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        if isinstance(obj, Decimal):
            return float(obj)
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        
        # å¤„ç†å®¹å™¨ç±»å‹
        if isinstance(obj, list):
            return [self.serialize_payload(item) for item in obj]
        if isinstance(obj, dict):
            return {k: self.serialize_payload(v) for k, v in obj.items()}
        
        # å¤„ç† Pydantic æ¨¡å‹
        if isinstance(obj, BaseModel):
            return obj.model_dump()
        
         # ğŸ‘‡ å…³é”®ä¿®å¤ï¼šå…¶ä»–ç±»å‹ï¼ˆstr, int, bool, None ç­‰ï¼‰åŸæ ·è¿”å›
        return obj
# å•ä¾‹å®ä¾‹ï¼ˆæ³¨æ„ï¼šåœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­æ…ç”¨ï¼‰
event_bus = EventPublisher(lifecycle_base_url=EVENTS_SERVICE_URL)