"""ç»Ÿä¸€è®°å¿†ç®¡ç†å™¨æ¨¡å—"""
from typing import Dict, Any, Optional, List, TYPE_CHECKING

if TYPE_CHECKING:
    from agents.tree.tree_manager import TreeManager
import time  # ç”¨äºæµ‹è¯•æ—¶ç­‰å¾… embedding å®Œæˆ

# ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
from ...capability_base import CapabilityBase
from .short_term import ShortTermMemory


# å¯¼å…¥ mem0
from mem0 import Memory
from env import MEM0_CONFIG

# === å…¨å±€å…±äº«çš„é‡é‡çº§èµ„æºï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰===
_SHARED_MEM0_CLIENT = None


def get_shared_mem0_client():
    """Lazy init mem0 client to avoid heavy work at import time."""
    global _SHARED_MEM0_CLIENT
    if _SHARED_MEM0_CLIENT is None:
        _SHARED_MEM0_CLIENT = Memory.from_config(MEM0_CONFIG)
    return _SHARED_MEM0_CLIENT


from datetime import datetime
import json
import os
import re
from .memory_interfaces import IVaultRepository, IProceduralRepository, IResourceRepository


class UnifiedMemoryManager():
    def __init__(self, 
                # user_id: str="default", 
                vault_repo: IVaultRepository=None,
                procedural_repo: IProceduralRepository=None,
                resource_repo: IResourceRepository=None,
                mem0_client=None,
                qwen_client=None
                ):
        # self.user_id = user_id
        self.mem0 = mem0_client
        self._mem0_warned = False
        self.stm = ShortTermMemory(max_history=10)  # ä»ä¿ç•™çŸ­æœŸå¯¹è¯å†å²
        self.qwen = qwen_client # â† å…³é”®ï¼
        # å„ä¸“ç”¨å­˜å‚¨ï¼ˆå¯ lazy initï¼‰
        self.vault_repo = vault_repo or create_vault_repo(config["vault"])
        self.procedural_repo = procedural_repo or create_procedural_repo(config["procedural"])
        self.resource_repo = resource_repo or create_resource_repo(config["resource"])
        self._core_cache = None

    # ======================
    # 1. å…­ç±»è®°å¿†å†™å…¥æ¥å£
    # ======================

    def add_memory_intelligently(self, user_id, content: str, metadata: Dict = None):
        """
        æ™ºèƒ½è®°å¿†è·¯ç”±ï¼š
        1. å…ˆå­˜å…¥çŸ­æœŸè®°å¿†
        2. è°ƒç”¨ Qwen åˆ†æå¹¶æ‹†è§£ä¸ºå¤šç±»é•¿æœŸè®°å¿†
        3. åˆ†åˆ«å†™å…¥å¯¹åº”å­˜å‚¨
        """
        print(f"ğŸ” [ADD] USER={user_id} | CONTENT='{content}'")
        if self.qwen is None:
            from ...registry import capability_registry
            from ...llm.interface import ILLMCapability
            self.qwen = capability_registry.get_capability(
                "llm", expected_type=ILLMCapability
            )
        # Step 1: å­˜å…¥çŸ­æœŸè®°å¿†ï¼ˆåŸå§‹å†…å®¹ï¼‰
        self.stm.add_message(user_id, "user", content)
        # Step 2: è°ƒç”¨ Qwen åˆ†ç±»
        prompt = self._build_memory_classification_prompt(content)
        try:
            response = self.qwen.generate(
                prompt=prompt,
                max_tokens=512,
                temperature=0.1,  # é™ä½éšæœºæ€§
                stop=["\n\n"]  # å¯é€‰
            )
            parsed = json.loads(response.strip())
        except Exception as e:
            print(f"[MemoryRouter] Qwen è§£æå¤±è´¥: {e}ï¼Œå›é€€ä¸º episodic")
            self.add_episodic_memory(user_id, content)
            return

        # Step 3: æŒ‰ç±»åˆ«å†™å…¥
        if "core" in parsed:
            for item in parsed["core"]:
                self.add_core_memory(user_id, item.strip())

        if "episodic" in parsed:
            for item in parsed["episodic"]:
                self.add_episodic_memory(user_id, item.strip())

        if "semantic" in parsed:
            for item in parsed["semantic"]:
                self.add_semantic_memory(user_id, item.strip())

        if "procedural" in parsed:
            for item in parsed["procedural"]:
                # ç®€åŒ–ï¼šå°†æ•´å¥ä½œä¸ºå•æ­¥æµç¨‹ï¼›è¿›é˜¶å¯è®© Qwen æ‹† steps
                self.add_procedural_memory(
                    user_id=user_id,
                    domain="general",
                    task_type="user_defined",
                    title=item[:50],  # æˆªå–æ ‡é¢˜
                    steps=[item.strip()]
                )

        if "resource" in parsed:
            for item in parsed["resource"]:
                # è¿›é˜¶ï¼šå¯ç”¨æ­£åˆ™æå–è·¯å¾„ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                self.add_resource_memory(
                    user_id=user_id,
                    file_path="mentioned_in_text",
                    summary=item.strip(),
                    doc_type="text"
                )

        if "vault" in parsed:
            for item in parsed["vault"]:
                # âš ï¸ å®‰å…¨å»ºè®®ï¼šä¸è¦ç›´æ¥å­˜å‚¨æ˜æ–‡ï¼è¿™é‡Œä»…ä¸ºæ¼”ç¤º
                self.add_vault_memory(
                    user_id=user_id,
                    category="sensitive_auto_detected",
                    key_name="auto_" + str(hash(item))[:8],
                    value=item.strip()
                )


    def _build_memory_classification_prompt(self, user_input: str) -> str:
        return f"""
    ä½ æ˜¯ä¸€ä¸ªé«˜çº§è®°å¿†ç®¡ç†ç³»ç»Ÿï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥æ™ºèƒ½æ‹†è§£ä¸ºå¤šä¸ªè®°å¿†ç‰‡æ®µï¼Œå¹¶åˆ†ç±»å­˜å‚¨åˆ°ä»¥ä¸‹å…­ç±»é•¿æœŸè®°å¿†ä¸­ï¼š

    - **core**: ç”¨æˆ·èº«ä»½ã€åå¥½ã€é•¿æœŸå±æ€§ï¼ˆå¦‚â€œæˆ‘æ˜¯è®¾è®¡å¸ˆâ€ã€â€œæˆ‘ä¸åƒé¦™èœâ€ï¼‰
    - **episodic**: å…·ä½“äº‹ä»¶ï¼Œå«æ—¶é—´/åœ°ç‚¹/äººç‰©ï¼ˆå¦‚â€œæ˜¨å¤©æˆ‘å»äº†ä¸Šæµ·å¼€ä¼šâ€ï¼‰
    - **semantic**: é€šç”¨çŸ¥è¯†ã€äº‹å®ã€æ¦‚å¿µï¼ˆå¦‚â€œå…‰é€Ÿæ˜¯ 3Ã—10^8 m/sâ€ï¼‰
    - **procedural**: æ“ä½œæ­¥éª¤ã€æ–¹æ³•ã€æµç¨‹ï¼ˆå¦‚â€œé‡è£…ç³»ç»Ÿè¦å…ˆå¤‡ä»½æ•°æ®â€ï¼‰
    - **resource**: æåˆ°çš„æ–‡ä»¶ã€é“¾æ¥ã€æ–‡æ¡£ï¼ˆå¦‚â€œè§é™„ä»¶ resume.pdfâ€ï¼‰
    - **vault**: æ•æ„Ÿä¿¡æ¯ï¼ˆå¯†ç ã€tokenã€èº«ä»½è¯ç­‰ï¼Œéœ€è°¨æ…å¤„ç†ï¼‰

    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä»…åŒ…å«å­˜åœ¨çš„ç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«å¯¹åº”ä¸€ä¸ª**å­—ç¬¦ä¸²åˆ—è¡¨**ï¼ˆå¯å¤šæ¡ï¼‰ï¼š

    {{
    "core": ["..."],
    "episodic": ["..."],
    "semantic": ["..."],
    "procedural": ["..."],
    "resource": ["..."],
    "vault": ["..."]
    }}

    æ³¨æ„ï¼š
    - ä¸è¦ç¼–é€ å†…å®¹ï¼Œåªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ä¿¡æ¯ã€‚
    - åŒä¸€å¥è¯çš„ä¸åŒéƒ¨åˆ†å¯å½’å±ä¸åŒç±»åˆ«ã€‚
    - è‹¥æŸç±»åˆ«æ— å†…å®¹ï¼Œåˆ™çœç•¥è¯¥å­—æ®µã€‚
    - ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—ï¼Œåªè¾“å‡º JSONã€‚

    ç”¨æˆ·è¾“å…¥ï¼š
    {user_input}
    """


    def add_core_memory(self, user_id,content: str):
        """æ ¸å¿ƒè®°å¿†ï¼šç”¨æˆ·åŸºæœ¬ä¿¡æ¯ã€åå¥½"""
        mem0 = self._ensure_mem0()
        if not mem0:
            return
        mem0.add(
            content,
            user_id=user_id,
            metadata={"type": "core", "updated_at": datetime.now().isoformat()}
        )
        self._core_memory_cache = None  # å¤±æ•ˆç¼“å­˜

    def add_episodic_memory(self, user_id,content: str, timestamp: str = None):
        """æƒ…æ™¯è®°å¿†ï¼šå…·ä½“äº‹ä»¶"""
        meta = {
            "type": "episodic",
            "timestamp": timestamp or datetime.now().isoformat()
        }
        mem0 = self._ensure_mem0()
        if not mem0:
            return
        mem0.add(content, user_id=user_id, metadata=meta)

    def add_vault_memory(self,user_id, category: str, key_name: str, value: str):
        self.vault_repo.store(user_id, category, key_name, value)

    def add_procedural_memory(self, user_id: str, domain: str, task_type: str, title: str, steps: List[str]):
        self.procedural_repo.add_procedure(user_id, domain, task_type, title, steps)

    def add_resource_memory(self, user_id: str, file_path: str, summary: str, doc_type: str = "pdf"):
        self.resource_repo.add_document(user_id, file_path, summary, doc_type)

    def add_semantic_memory(self, user_id: str, content: str, category: str = ""):
        """è¯­ä¹‰è®°å¿†ï¼šäº‹å®æ€§çŸ¥è¯†"""
        meta = {"type": "semantic"}
        if category: meta["category"] = category
        mem0 = self._ensure_mem0()
        if not mem0:
            return
        mem0.add(content, user_id=user_id, metadata=meta)

    # ======================
    # 2. è®°å¿†æ£€ç´¢æ¥å£ï¼ˆæŒ‰ç±»å‹ï¼‰
    # ======================

    def _search_by_type(self, user_id: str, memory_type: str, query: str = "", limit: int = 5):
        mem0 = self._ensure_mem0()
        if not mem0:
            return []
        filters = {"type": memory_type}
        if not query:
            query = "relevant information"  # Mem0 è¦æ±‚ query éç©º
        results = mem0.search(
            user_id=user_id,
            query=query,
            filters=filters,
            limit=limit
        )
        return [r.get("memory", "") for r in results.get("results", [])]

    def get_core_memory(self, user_id: str) -> str:
        """è·å–æ ¸å¿ƒè®°å¿†ï¼ˆç¼“å­˜ä¼˜åŒ–ï¼‰"""
        if not self._ensure_mem0():
            return ""
        if self._core_cache is None:
            memories = self._search_by_type(user_id, "core", limit=10)
            self._core_cache = "\n".join(memories) if memories else ""
        return self._core_cache

    def _ensure_mem0(self):
        if self.mem0 is None:
            if not self._mem0_warned:
                print("[Memory] mem0 disabled or unavailable, skipping mem0 operations.")
                self._mem0_warned = True
            return None
        return self.mem0

    def get_episodic_memory(self, user_id: str, query: str, limit: int = 3) -> str:
        return "\n".join(self._search_by_type(user_id, "episodic", query, limit))

    # ä¿®æ”¹æ£€ç´¢æ–¹æ³•
    def get_vault_memory(self, user_id: str, category: str = None) -> str:
        items = self.vault_repo.retrieve(user_id, category)
        return "\n".join(items)

    def get_procedural_memory(self, user_id: str, query: str, domain: str = None, limit: int = 2) -> str:
        results = self.procedural_repo.search(user_id, query, domain=domain, limit=limit)
        return "\n\n".join(results)

    def get_resource_memory(self, user_id: str, query: str) -> str:
        docs = self.resource_repo.search(user_id, query, limit=2)
        return "\n".join([
            f"[{d['filename']}]: {d['summary']} (ID: {d['id']})"
            for d in docs
        ])
    # ======================
    # 3. ä¸Šä¸‹æ–‡æ„å»ºï¼ˆä¾› LLM ä½¿ç”¨ï¼‰
    # ======================

    def _generate_retrieval_plan(self, goal: str, scene: str) -> Dict[str, str]:
        """ä½¿ç”¨ Qwen åŠ¨æ€ç”Ÿæˆå¤šç±»å‹è®°å¿†çš„æ£€ç´¢æŸ¥è¯¢"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªé«˜çº§è®°å¿†ç³»ç»Ÿè°ƒåº¦å™¨ã€‚è¯·æ ¹æ®ä»¥ä¸‹åœºæ™¯å’Œç›®æ ‡ï¼Œä¸ºå…­ç±»è®°å¿†ç”Ÿæˆæœ€ç›¸å…³çš„æ£€ç´¢å…³é”®è¯æˆ–çŸ­å¥ã€‚
    ä»…è¾“å‡º JSONï¼ŒåŒ…å«éœ€è¦æ£€ç´¢çš„ç±»åˆ«åŠå…¶æŸ¥è¯¢è¯­å¥ã€‚ä¸è¦è§£é‡Šï¼Œä¸è¦å¤šä½™å­—æ®µã€‚

    åœºæ™¯ï¼š{scene}
    ç›®æ ‡ï¼š{goal}

    è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
    {{"core": "ç”¨æˆ·å§“åå’ŒèŒä¸šåå¥½", "episodic": "æœ€è¿‘ä¸€æ¬¡å‡ºå·®æˆ–é¡¹ç›®ç»å†"}}

    ä½ çš„è¾“å‡ºï¼š
    """
        try:
            resp = self.qwen.generate(prompt, max_tokens=256, temperature=0.1)
            return json.loads(resp.strip())
        except Exception as e:
            # fallback plan
            return {
                "core": goal,
                "episodic": goal,
                "semantic": goal,
                "procedural": goal,
                "resource": goal
            }

    def _execute_retrieval_plan(self, user_id: str, plan: Dict[str, str]) -> Dict[str, str]:
        """æ‰§è¡Œæ£€ç´¢è®¡åˆ’ï¼Œè¿”å›åŸå§‹è®°å¿†ç‰‡æ®µå­—å…¸"""
        results = {}

        if "core" in plan:
            core = self.get_core_memory(user_id)
            if core:
                results["core"] = core

        if "episodic" in plan:
            episodic = self.get_episodic_memory(user_id, plan["episodic"], limit=3)
            if episodic:
                results["episodic"] = episodic

        if "semantic" in plan:
            semantic = "\n".join(self._search_by_type(user_id, "semantic", plan["semantic"], limit=3))
            if semantic:
                results["semantic"] = semantic

        if "procedural" in plan:
            procedural = self.get_procedural_memory(user_id, plan["procedural"], domain=None, limit=3)
            if procedural:
                results["procedural"] = procedural

        if "resource" in plan:
            return
            resource = self.get_resource_memory(user_id, plan["resource"], limit=3)
            if resource:
                results["resource"] = resource

        # vault ä¸åœ¨æ­¤å¤„è‡ªåŠ¨æ£€ç´¢ï¼ˆå®‰å…¨åŸå› ï¼‰ï¼Œç”± build_execution_context æ˜¾å¼æ§åˆ¶

        return results

    def _synthesize_context_with_qwen(self, user_id: str, raw_memories: Dict[str, str], scene: str, include_vault: bool = False) -> str:
        """ä½¿ç”¨ Qwen åˆæˆæœ€ç»ˆä¸Šä¸‹æ–‡ï¼Œè‡ªåŠ¨è„±æ•"""
        # è·å– vaultï¼ˆä»…å½“æ˜¾å¼å…è®¸ï¼‰
        if include_vault:
            vault = self.get_vault_memory(user_id)
            if vault:
                # ç®€å•è„±æ•ï¼šæ›¿æ¢ token / å¯†ç ç­‰ï¼ˆå¯æ‰©å±•æ­£åˆ™ï¼‰
                vault = re.sub(r'(?i)(token|password|key|secret)[:\s]*[\'"]?[\w\-_\.]+[\'"]?', r'\1: [REDACTED]', vault)
                raw_memories["vault"] = vault

        if not raw_memories:
            return "æ— ç›¸å…³è®°å¿†å¯ç”¨ã€‚"

        memory_blocks = "\n\n".join(f"[{k.upper()} MEMORY]\n{v}" for k, v in raw_memories.items())

        prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹çš„è®°å¿†æ•´åˆæ¨¡å—ã€‚è¯·å°†ä»¥ä¸‹è®°å¿†ç‰‡æ®µæ•´åˆæˆä¸€æ®µç®€æ´ã€è¿è´¯ã€é€‚åˆç”¨äºã€Œ{scene}ã€çš„ä¸Šä¸‹æ–‡æè¿°ã€‚

    è¦æ±‚ï¼š
    - ä¿ç•™æ‰€æœ‰å…³é”®äº‹å®ï¼ˆå¦‚å§“åã€æ—¶é—´ã€æ–‡ä»¶åã€æ“ä½œæ­¥éª¤ï¼‰
    - åˆå¹¶é‡å¤æˆ–ç›¸ä¼¼å†…å®¹
    - ä½¿ç”¨è‡ªç„¶è¯­è¨€ï¼Œé¿å…æ ‡ç­¾å¦‚ [CORE MEMORY]
    - æ•æ„Ÿä¿¡æ¯å¿…é¡»æ˜¾ç¤ºä¸º [REDACTED]
    - ä¸è¦ç¼–é€ æœªæåŠçš„ä¿¡æ¯
    - è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦ Markdown

    è®°å¿†å†…å®¹ï¼š
    {memory_blocks}

    æ•´åˆåçš„ä¸Šä¸‹æ–‡ï¼š
    """
        try:
            synthesized = self.qwen.generate(prompt, max_tokens=512, temperature=0.3)
            return synthesized.strip()
        except Exception as e:
            # fallback: ç›´æ¥æ‹¼æ¥ï¼ˆä¸åˆæˆï¼‰
            return "\n\n".join(raw_memories.values())


    # ======================
    # 3. æ™ºèƒ½ä¸Šä¸‹æ–‡æ„å»ºï¼ˆæŒ‰åœºæ™¯ï¼ŒQwen å…¨ç¨‹é©±åŠ¨ï¼‰
    # ======================

    def build_conversation_context(self, user_id: str, current_input: str = "") -> str:
        """
        åœºæ™¯1ï¼šå¯¹è¯ç†è§£ & ä»»åŠ¡é€‰æ‹©
        - Qwen åŠ¨æ€å†³å®šæŸ¥å“ªäº›è®°å¿†
        - åˆæˆè‡ªç„¶è¯­è¨€ä¸Šä¸‹æ–‡ä¾› LLM ç†è§£ç”¨æˆ·æ„å›¾
        """
        goal = current_input or "å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡"
        plan = self._generate_retrieval_plan(goal, scene="å¯¹è¯ç†è§£ä¸ä»»åŠ¡é€‰æ‹©")
        raw = self._execute_retrieval_plan(user_id, plan)
        
        # åŠ å…¥çŸ­æœŸå¯¹è¯å†å²ï¼ˆå§‹ç»ˆéœ€è¦ï¼‰
        chat_hist = ""
        if user_id and user_id.count(":") == 1:
            chat_hist = self.stm.format_history_by_scope(user_id, n=6)
        else:
            chat_hist = self.stm.format_history(user_id, n=6)
        if chat_hist.strip():
            raw["short_term"] = f"[è¿‘æœŸå¯¹è¯]\n{chat_hist}"

        return self._synthesize_context_with_qwen(user_id, raw, scene="å¯¹è¯ç†è§£")


    def build_planning_context(self, user_id: str, planning_goal: str) -> str:
        """
        åœºæ™¯2ï¼šä»»åŠ¡è§„åˆ’ä¸æµç¨‹ç¼–æ’
        - é‡ç‚¹è·å– proceduralã€episodicã€resource
        - åˆæˆåç”¨äºä»»åŠ¡åˆ†è§£ä¸æ’åº
        """
        plan = self._generate_retrieval_plan(planning_goal, scene="å¤šä»»åŠ¡è§„åˆ’ä¸è°ƒåº¦")
        raw = self._execute_retrieval_plan(user_id, plan)
        return self._synthesize_context_with_qwen(user_id, raw, scene="ä»»åŠ¡è§„åˆ’")


    def build_execution_context(self, user_id: str, task_description: str, include_sensitive: bool = False) -> str:
        """
        åœºæ™¯3ï¼šä»»åŠ¡æ‰§è¡Œå‰å¢å¼º
        - è¡¥å……å†å²ç»éªŒã€æ ‡å‡†æµç¨‹ã€å‚è€ƒèµ„æ–™
        - å¯é€‰åŒ…å« vaultï¼ˆè‡ªåŠ¨è„±æ•ï¼‰
        """
        plan = self._generate_retrieval_plan(task_description, scene="å…·ä½“ä»»åŠ¡æ‰§è¡Œå‡†å¤‡")
        raw = self._execute_retrieval_plan(user_id, plan)
        return self._synthesize_context_with_qwen(
            user_id,
            raw,
            scene="ä»»åŠ¡æ‰§è¡Œ",
            include_vault=include_sensitive
        )

    # ======================
    # 4. è¯­ä¹‰æŒ‡é’ˆè¡¥å…¨ï¼šçˆ¶çº§è®°å¿†å›æº¯
    # ======================

    def get_ancestor_context(
        self,
        user_id: str,
        agent_id: str,
        tree_manager: Any,
        max_levels: int = 3,
        query: str = ""
    ) -> List[Dict[str, Any]]:
        """
        æ²¿æ ‘å‘ä¸Šå›æº¯ï¼Œæ”¶é›†çˆ¶çº§ Agent çš„ä¸šåŠ¡è®°å¿†ï¼Œç”¨äºæ¶ˆè§£ä»£è¯æ­§ä¹‰ã€‚

        æœºåˆ¶ï¼š
        - ä»å½“å‰ agent_id å¼€å§‹ï¼Œæ²¿æ ‘å‘ä¸Šéå†çˆ¶çº§ Agent
        - æ£€ç´¢æ¯çº§çš„å¯¹è¯å†å²å’Œæ ¸å¿ƒè®°å¿†
        - è¿”å›æŒ‰å±‚çº§æ’åºçš„ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆè¿‘åˆ°è¿œï¼‰

        Args:
            user_id: ç”¨æˆ·ID
            agent_id: å½“å‰ Agent ID
            tree_manager: TreeManager å®ä¾‹ï¼Œç”¨äºè·å–çˆ¶çº§å…³ç³»
            max_levels: æœ€å¤§å›æº¯å±‚æ•°ï¼Œé»˜è®¤ 3 å±‚
            query: å¯é€‰çš„æŸ¥è¯¢å…³é”®è¯ï¼Œç”¨äºç›¸å…³æ€§è¿‡æ»¤

        Returns:
            List[Dict]: æ¯çº§çˆ¶èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            [
                {
                    "agent_id": "parent_agent_1",
                    "level": 1,
                    "conversation_history": "...",
                    "core_memory": "...",
                    "agent_description": "...",
                    "task_goal": "..."
                },
                ...
            ]
        """
        if not tree_manager:
            return []

        ancestor_contexts = []
        current_id = agent_id
        level = 0

        while level < max_levels:
            # è·å–çˆ¶èŠ‚ç‚¹
            parent_id = tree_manager.get_parent(current_id)
            if not parent_id:
                break  # å·²åˆ°è¾¾æ ¹èŠ‚ç‚¹

            level += 1

            # æ„å»ºçˆ¶èŠ‚ç‚¹çš„è®°å¿† scope
            # æ ¼å¼: user_id:root_agent_id:parent_agent_id
            root_path = tree_manager.get_full_path(parent_id)
            root_agent_id = root_path[0] if root_path else parent_id
            parent_scope = f"{user_id}:{root_agent_id}:{parent_id}"

            # è·å–çˆ¶èŠ‚ç‚¹çš„å¯¹è¯å†å²
            conversation_history = ""
            try:
                conversation_history = self.stm.format_history_by_scope(parent_scope, n=5)
                if not conversation_history:
                    # å°è¯•æ›´å®½æ¾çš„ scope
                    broader_scope = f"{user_id}:{root_agent_id}"
                    conversation_history = self.stm.format_history_by_scope(broader_scope, n=5)
            except Exception as e:
                print(f"[AncestorContext] Failed to get conversation history for {parent_id}: {e}")

            # è·å–çˆ¶èŠ‚ç‚¹çš„æ ¸å¿ƒè®°å¿†
            core_memory = ""
            try:
                core_memory = self.get_core_memory(user_id)
            except Exception as e:
                print(f"[AncestorContext] Failed to get core memory for {parent_id}: {e}")

            # è·å–çˆ¶èŠ‚ç‚¹çš„å…ƒæ•°æ®ï¼ˆæè¿°ã€ä»»åŠ¡ç›®æ ‡ç­‰ï¼‰
            agent_description = ""
            task_goal = ""
            try:
                parent_meta = tree_manager.get_agent_meta(parent_id)
                if parent_meta:
                    agent_description = parent_meta.get("description", "")
                    # å°è¯•ä» datascope æˆ– capability ä¸­æå–ä»»åŠ¡ç›®æ ‡
                    datascope = parent_meta.get("datascope") or parent_meta.get("data_scope", "")
                    capability = parent_meta.get("capability", "")
                    if datascope:
                        task_goal = f"æ•°æ®èŒƒå›´: {datascope}"
                    if capability:
                        task_goal += f" èƒ½åŠ›: {capability}"
            except Exception as e:
                print(f"[AncestorContext] Failed to get agent meta for {parent_id}: {e}")

            # å¦‚æœæœ‰æŸ¥è¯¢å…³é”®è¯ï¼Œè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤
            if query:
                # ç®€å•çš„å…³é”®è¯åŒ¹é…è¿‡æ»¤
                combined_text = f"{conversation_history} {core_memory} {agent_description} {task_goal}"
                query_keywords = set(query.lower().split())
                combined_lower = combined_text.lower()
                relevance_score = sum(1 for kw in query_keywords if kw in combined_lower)

                # å¦‚æœå®Œå…¨ä¸ç›¸å…³ï¼Œè·³è¿‡è¿™ä¸€å±‚
                if relevance_score == 0 and level > 1:
                    current_id = parent_id
                    continue

            ancestor_context = {
                "agent_id": parent_id,
                "level": level,
                "conversation_history": conversation_history,
                "core_memory": core_memory,
                "agent_description": agent_description,
                "task_goal": task_goal
            }

            ancestor_contexts.append(ancestor_context)
            current_id = parent_id

        return ancestor_contexts

    def build_ancestor_context_summary(
        self,
        user_id: str,
        agent_id: str,
        tree_manager: Any,
        max_levels: int = 3,
        query: str = ""
    ) -> str:
        """
        æ„å»ºçˆ¶çº§ä¸Šä¸‹æ–‡çš„æ‘˜è¦æ–‡æœ¬ï¼Œç”¨äºæ³¨å…¥åˆ° LLM prompt ä¸­ã€‚

        Args:
            user_id: ç”¨æˆ·ID
            agent_id: å½“å‰ Agent ID
            tree_manager: TreeManager å®ä¾‹
            max_levels: æœ€å¤§å›æº¯å±‚æ•°
            query: å¯é€‰çš„æŸ¥è¯¢å…³é”®è¯

        Returns:
            str: æ ¼å¼åŒ–çš„çˆ¶çº§ä¸Šä¸‹æ–‡æ‘˜è¦
        """
        ancestors = self.get_ancestor_context(
            user_id=user_id,
            agent_id=agent_id,
            tree_manager=tree_manager,
            max_levels=max_levels,
            query=query
        )

        if not ancestors:
            return ""

        summary_parts = []
        for ctx in ancestors:
            level = ctx["level"]
            agent_id = ctx["agent_id"]

            parts = [f"ã€çˆ¶çº§ {level} - {agent_id}ã€‘"]

            if ctx.get("agent_description"):
                parts.append(f"æè¿°: {ctx['agent_description']}")

            if ctx.get("task_goal"):
                parts.append(f"ä»»åŠ¡ç›®æ ‡: {ctx['task_goal']}")

            if ctx.get("conversation_history"):
                # æˆªå–æœ€è¿‘çš„å¯¹è¯ï¼Œé¿å…è¿‡é•¿
                history = ctx["conversation_history"]
                if len(history) > 500:
                    history = history[-500:] + "..."
                parts.append(f"è¿‘æœŸå¯¹è¯:\n{history}")

            if ctx.get("core_memory"):
                memory = ctx["core_memory"]
                if len(memory) > 300:
                    memory = memory[:300] + "..."
                parts.append(f"æ ¸å¿ƒè®°å¿†: {memory}")

            summary_parts.append("\n".join(parts))

        return "\n\n".join(summary_parts)
