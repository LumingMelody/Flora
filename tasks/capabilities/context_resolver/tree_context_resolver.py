"""ä¸Šä¸‹æ–‡è§£æå™¨å®ç°"""
from typing import Dict, Any, List, Optional, Tuple
from ..capability_base import CapabilityBase
import logging
import json
import re
from .interface import IContextResolverCapbility 
import logging
logger = logging.getLogger(__name__)

class TreeContextResolver(IContextResolverCapbility):
    """
    å…·ä½“çš„å®ç°ç±»ï¼š
    ä¸ TreeManager é›†æˆï¼Œåˆ©ç”¨æ ‘å½¢ç»“æ„è¿›è¡Œè¯­ä¹‰åŒ–çš„å±‚çº§æœç´¢ã€‚
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.config = {}
        
        # ä¾èµ–é¡¹ï¼šç°åœ¨ä½¿ç”¨ tree_manager
        self.tree_manager = None 
        self.llm_client = None
        
        self.variable_pattern = re.compile(r'\$\{([^}]+)\}')
        self.context_templates = {}

    def get_capability_type(self) -> str:
        return 'tree_context_resolver'

    def initialize(self, config: Dict[str, Any]) -> None:
        self.config = config
        self.logger.info("TreeContextResolver initialized with config.")

    def shutdown(self) -> None:
        self.context_templates.clear()
        self.tree_manager = None
        self.logger.info("TreeContextResolver shutdown.")

    def set_dependencies(self, tree_manager: Any=None, llm_client: Any = None) -> None:
        """
        æ³¨å…¥ TreeManager å•ä¾‹å’Œ LLM å®¢æˆ·ç«¯
        """
        if tree_manager:
            self.tree_manager = tree_manager
        else:
            from agents.tree.tree_manager import treeManager
            self.tree_manager=treeManager
        if llm_client:
            self.llm_client = llm_client
        else:
            from ..llm.interface import ILLMCapability
            from .. import get_capability
            self.llm_client:ILLMCapability = get_capability("llm",ILLMCapability)
        
        self.logger.info("Dependencies (TreeManager, LLM) injected.")

    # ----------------------------------------------------------
    # æ ¸å¿ƒé€»è¾‘ï¼šåŸºäº TreeManager çš„å¯»å€
    # ----------------------------------------------------------

    def resolve_context(self, context_requirements: Dict[str, str], agent_id: str) -> Dict[str, Any]:
        """
        è§£æä¸Šä¸‹æ–‡éœ€æ±‚ï¼š
        1. å…ˆé€šè¿‡ _resolve_kv_via_layered_search å®šä½æ•°æ®æ‰€åœ¨ä½ç½®ï¼ˆåº“/è¡¨/åˆ—ï¼‰ï¼›
        2. è‹¥å®šä½æˆåŠŸï¼Œåˆ™ä½¿ç”¨ VannaTextToSQL æ‰§è¡ŒçœŸå®æŸ¥è¯¢ï¼Œè¿”å›å®é™…æ•°æ®ã€‚
        """
        if not self.tree_manager or not self.llm_client:
            self.set_dependencies()

        result = {}
        try:
            path = self.tree_manager.get_full_path(agent_id)
            path_str = " -> ".join(path)
        except:
            path_str = agent_id

        self.logger.info(f"Start resolving context for agent: {agent_id} (Path: {path_str})")

        # è·å–å½“å‰ Agent çš„åŸºç¡€å…ƒä¿¡æ¯ï¼ˆç”¨äº fallback æˆ–æ—¥å¿—ï¼‰
        base_agent_meta = {}
        try:
            base_agent_meta = self.tree_manager.get_agent_meta(agent_id) or {}
        except Exception as e:
            self.logger.warning(f"Could not retrieve base agent meta for {agent_id}: {e}")

        for key, value_desc in context_requirements.items():
            try:
                query = f"éœ€æŸ¥æ‰¾æ•°æ®: '{key}', ä¸šåŠ¡æè¿°: '{value_desc}'"
                
                # Step 1: å®šä½æ•°æ®ä½ç½®ï¼ˆåº“ã€è¡¨ã€åˆ—ç­‰ï¼‰
                leaf_meta = self._resolve_kv_via_layered_search(agent_id, query, key)
                if not leaf_meta:
                    leaf_meta = self._resolve_kv_globally(query)

                
                if not leaf_meta:
                    self.logger.warning(f"âŒ Unresolved '{key}' (Desc: {value_desc}) â€“ no location found")
                    result[key] = None
                    continue

                # Step 2: å¦‚æœå®šä½æˆåŠŸï¼Œå°è¯•ç”¨ Vanna æŸ¥è¯¢çœŸå®æ•°æ®
                self.logger.info(f"ğŸ“ Located '{key}' at: {leaf_meta}")
                
                # æ„é€  Vanna æ‰€éœ€çš„ agent_meta æ ¼å¼ï¼šdatabase = "db.table"
                db_name = leaf_meta.get("database") or leaf_meta.get("db")
                table_name = leaf_meta.get("table") or leaf_meta.get("tbl")

                # Some nodes store "db.table" in database field.
                if db_name and not table_name and "." in str(db_name):
                    parts = str(db_name).split(".", 1)
                    db_name = parts[0].strip() or None
                    table_name = parts[1].strip() or None

                if not db_name or not table_name:
                    db_name, table_name = self._extract_db_table_from_meta(leaf_meta)
                
                if not db_name or not table_name:
                    self.logger.warning(f"âš ï¸ Incomplete location info for '{key}': {leaf_meta}, skip Vanna query")
                    result[key] = None
                    continue

                vanna_agent_meta = {
                    "database": f"{db_name}.{table_name}",
                    "database_type": leaf_meta.get("database_type", base_agent_meta.get("database_type", "mysql"))
                }

                # åˆå§‹åŒ– Vanna èƒ½åŠ›
                from .. import get_capability
                from ..text_to_sql.text_to_sql import ITextToSQLCapability
                try:
                    text_to_sql_cap: ITextToSQLCapability = get_capability(
                        "text_to_sql", ITextToSQLCapability
                    )
                except Exception as e:
                    self.logger.warning(f"Text-to-SQL capability unavailable: {e}")
                    result[key] = None
                    continue

                text_to_sql_cap.initialize({
                    "agent_id": agent_id,
                    "agent_meta": vanna_agent_meta
                })

                try:
                    # ä½¿ç”¨åŸå§‹ä¸šåŠ¡æè¿°ä½œä¸ºæŸ¥è¯¢è¯­å¥
                    response = text_to_sql_cap.execute_query(user_query=value_desc, context=None)
                    records = response.get("result", [])
                    
                    if records:
                        # ä½¿ç”¨ LLM ä»æŸ¥è¯¢ç»“æœä¸­æå–ç¬¦åˆä¸šåŠ¡éœ€æ±‚çš„å€¼
                        resolved_value = self._extract_value_from_records(
                            key=key,
                            value_desc=value_desc,
                            records=records
                        )
                        result[key] = resolved_value
                        self.logger.info(f"âœ… Resolved '{key}' with real data (rows: {len(records)}, extracted: {type(resolved_value).__name__})")
                    else:
                        self.logger.warning(f"ğŸ” Located but no data returned for '{key}'")
                        result[key] = None  # æˆ–ä¿ç•™ leaf_metaï¼Œè§†ä¸šåŠ¡è€Œå®š
                        
                finally:
                    # ç¡®ä¿é‡Šæ”¾èµ„æº
                    text_to_sql_cap.shutdown()

            except Exception as e:
                self.logger.error(f"Error resolving key '{key}': {str(e)}", exc_info=True)
                result[key] = None

        return result

    def _extract_db_table_from_meta(self, meta: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:
        """
        Try to extract database/table info from datascope or other metadata.
        """
        db_name = None
        table_name = None
        datascope = meta.get("datascope") or meta.get("data_scope")

        if isinstance(datascope, str) and datascope.strip():
            try:
                datascope = json.loads(datascope)
            except Exception:
                # Fallback: allow "db.table" literal in datascope string.
                if "." in datascope:
                    parts = datascope.split(".", 1)
                    db_name = parts[0].strip() or None
                    table_name = parts[1].strip() or None

        if isinstance(datascope, dict):
            db_name = db_name or datascope.get("database") or datascope.get("db") or datascope.get("schema")
            table_name = (
                table_name
                or datascope.get("table")
                or datascope.get("tbl")
                or datascope.get("table_name")
            )
            # Allow database value to be "db.table".
            if db_name and not table_name and "." in str(db_name):
                parts = str(db_name).split(".", 1)
                db_name = parts[0].strip() or None
                table_name = parts[1].strip() or None

        return db_name, table_name
    
    def _extract_value_from_records(
        self,
        key: str,
        value_desc: str,
        records: list
    ) -> Any:
        """
        ä½¿ç”¨ LLM ä» SQL æŸ¥è¯¢ç»“æœä¸­æå–ç¬¦åˆä¸šåŠ¡éœ€æ±‚çš„å€¼ã€‚

        Args:
            key: å‚æ•°åç§°ï¼Œå¦‚ "user_id"
            value_desc: ä¸šåŠ¡æè¿°ï¼Œå¦‚ "å½“å‰ç™»å½•ç”¨æˆ·çš„ID"
            records: SQL æŸ¥è¯¢è¿”å›çš„è®°å½•åˆ—è¡¨

        Returns:
            æå–åçš„å€¼ï¼Œå¯èƒ½æ˜¯å•å€¼ã€åˆ—è¡¨æˆ–å­—å…¸
        """
        # å¿«é€Ÿè·¯å¾„ï¼šå•è¡Œå•åˆ—ç›´æ¥è¿”å›
        if len(records) == 1:
            row = records[0]
            if isinstance(row, dict) and len(row) == 1:
                # å•è¡Œå•åˆ—ï¼Œç›´æ¥è¿”å›å€¼
                return list(row.values())[0]
            elif not isinstance(row, dict):
                # å•ä¸ªå€¼
                return row

        # å¿«é€Ÿè·¯å¾„ï¼šå•åˆ—å¤šè¡Œï¼Œè¿”å›å€¼åˆ—è¡¨
        if records and isinstance(records[0], dict) and len(records[0]) == 1:
            col_name = list(records[0].keys())[0]
            values = [r.get(col_name) for r in records if r.get(col_name) is not None]
            if len(values) == 1:
                return values[0]
            return values

        # å¤æ‚æƒ…å†µï¼šå¤šè¡Œå¤šåˆ—ï¼Œä½¿ç”¨ LLM æå–
        if not self.llm_client:
            self.set_dependencies()
            if not self.llm_client:
                self.logger.warning("LLM client unavailable, returning first record")
                return records[0] if len(records) == 1 else records

        # é™åˆ¶è®°å½•æ•°é‡ï¼Œé¿å… prompt è¿‡é•¿
        max_records = 20
        truncated = records[:max_records]
        truncated_note = f"ï¼ˆä»…å±•ç¤ºå‰ {max_records} æ¡ï¼Œå…± {len(records)} æ¡ï¼‰" if len(records) > max_records else ""
        logger.info(f"LLM prompting with {truncated} records")

        # æ ¼å¼åŒ–è®°å½•ä¸ºå¯è¯»æ–‡æœ¬ï¼ˆå¤„ç†ä¸å¯åºåˆ—åŒ–çš„ç±»å‹ï¼‰
        def make_json_serializable(obj):
            """é€’å½’å¤„ç†ä¸å¯ JSON åºåˆ—åŒ–çš„ç±»å‹"""
            if isinstance(obj, bytes):
                # bytes è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆå°è¯• UTF-8 è§£ç ï¼Œå¤±è´¥åˆ™ç”¨ hexï¼‰
                try:
                    return obj.decode('utf-8')
                except UnicodeDecodeError:
                    return obj.hex()
            elif isinstance(obj, dict):
                return {k: make_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_json_serializable(item) for item in obj]
            elif hasattr(obj, 'isoformat'):
                # datetime/date/time å¯¹è±¡
                return obj.isoformat()
            elif hasattr(obj, '__dict__'):
                # å…¶ä»–å¯¹è±¡å°è¯•è½¬ä¸ºå­—å…¸
                return make_json_serializable(obj.__dict__)
            else:
                return obj

        serializable_records = make_json_serializable(truncated)
        records_text = json.dumps(serializable_records, ensure_ascii=False, indent=2)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œä» SQL æŸ¥è¯¢ç»“æœä¸­æå–å‡ºæœ€åˆé€‚çš„å€¼ã€‚

ã€ä¸šåŠ¡éœ€æ±‚ã€‘
å‚æ•°å: {key}
æè¿°: {value_desc}

ã€SQL æŸ¥è¯¢ç»“æœã€‘{truncated_note}
{records_text}

ã€æå–è§„åˆ™ã€‘
1. æ ¹æ®ä¸šåŠ¡æè¿°åˆ¤æ–­éœ€è¦çš„æ˜¯å•ä¸ªå€¼ã€å¤šä¸ªå€¼è¿˜æ˜¯å®Œæ•´è®°å½•
2. å¦‚æœéœ€è¦å•ä¸ªå€¼ï¼ˆå¦‚ IDã€åç§°ï¼‰ï¼Œç›´æ¥è¾“å‡ºè¯¥å€¼
3. å¦‚æœéœ€è¦å¤šä¸ªå€¼ï¼ˆå¦‚ ID åˆ—è¡¨ï¼‰ï¼Œè¾“å‡º JSON æ•°ç»„æ ¼å¼
4. å¦‚æœéœ€è¦å®Œæ•´è®°å½•ï¼Œè¾“å‡º JSON å¯¹è±¡æˆ–æ•°ç»„
5. å¦‚æœæŸ¥è¯¢ç»“æœä¸ä¸šåŠ¡éœ€æ±‚ä¸åŒ¹é…ï¼Œè¾“å‡º null

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡ºæå–çš„å€¼ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ç›´æ¥è¾“å‡ºï¼Œå¦‚æœæ˜¯å¤æ‚ç»“æ„è¾“å‡º JSONã€‚
"""

        try:
            response = self.llm_client.generate(prompt=prompt)
            result_text = response.strip() if isinstance(response, str) else str(response).strip()

            # å°è¯•è§£æä¸º JSON
            if result_text.lower() == "null":
                return None

            # å°è¯• JSON è§£æ
            try:
                parsed = json.loads(result_text)
                return parsed
            except json.JSONDecodeError:
                # ä¸æ˜¯ JSONï¼Œä½œä¸ºå­—ç¬¦ä¸²è¿”å›
                # å»é™¤å¯èƒ½çš„å¼•å·
                if result_text.startswith('"') and result_text.endswith('"'):
                    return result_text[1:-1]
                if result_text.startswith("'") and result_text.endswith("'"):
                    return result_text[1:-1]
                return result_text

        except Exception as e:
            self.logger.error(f"LLM extraction failed for '{key}': {e}")
            # é™çº§ï¼šè¿”å›ç¬¬ä¸€æ¡è®°å½•æˆ–å…¨éƒ¨
            return records[0] if len(records) == 1 else records


    def _resolve_kv_via_layered_search(self, start_agent_id: str, query: str, key: str) -> Optional[Dict]:
        """
        é€‚é… TreeManager çš„å±‚çº§æœç´¢ç®—æ³•
        """
        # 1. åˆå§‹å®šä½ï¼šè·å– start_agent çš„çˆ¶èŠ‚ç‚¹ï¼Œä»¥ç¡®å®šåˆå§‹çš„"å…„å¼Ÿå±‚"
        parent_id = self.tree_manager.get_parent(start_agent_id)
        
        # ç”¨äºé˜²æ­¢æ­»å¾ªç¯ï¼ˆè™½ç„¶ TreeManager å†…éƒ¨æœ‰é˜²ç¯ï¼Œä½†æœç´¢é€»è¾‘å±‚ä¹Ÿä¿ç•™ä¸€ä»½ä¿é™©ï¼‰
        visited_layers = set()
        
        # è®°å½•å½“å‰è§†è§’çš„èŠ‚ç‚¹ï¼Œç”¨äºå‘ä¸Šå›æº¯æ—¶å®šä½
        current_focus_node = start_agent_id

        while True:
            # --- 1. ç¡®å®šå½“å‰æœç´¢å±‚ (Layer) ---
            if parent_id is None:
                # æ ¸å¿ƒå˜æ›´ï¼šåˆ©ç”¨ TreeManager.get_root_agents() è·å–æ ¹å±‚
                self.logger.debug(f"Searching Root Layer for: {key}")
                current_layer = self.tree_manager.get_root_agents()
                
                # å¦‚æœå½“å‰èšç„¦çš„èŠ‚ç‚¹æœ¬èº«å°±æ˜¯æ ¹èŠ‚ç‚¹ï¼Œä¸”åœ¨æ ¹å±‚ä¹Ÿæ‰¾ä¸åˆ°ï¼Œå¾ªç¯é€šå¸¸ä¼šåœ¨åé¢ Break
            else:
                # è·å–çˆ¶èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ï¼ˆå³å½“å‰å±‚ï¼‰
                current_layer = self.tree_manager.get_children(parent_id)

            # --- é˜²æ­»å¾ªç¯æ£€æŸ¥ ---
            layer_sig = tuple(sorted(current_layer))
            if layer_sig in visited_layers:
                self.logger.warning("Cycle detected in search layer. Stopping.")
                break
            visited_layers.add(layer_sig)

            # --- 2. åœ¨å½“å‰å±‚è¿›è¡Œè¯­ä¹‰åŒ¹é… ---
            matched_node_id = self._semantic_match_for_layer(query, current_layer)

            # --- 3. åŒ¹é…ç»“æœå¤„ç† ---
            if matched_node_id:
                # >> å‘½ä¸­åˆ†æ”¯ >>
                # ä½¿ç”¨ TreeManager è·å–å…ƒæ•°æ®
                node_meta = self.tree_manager.get_agent_meta(matched_node_id)
                
                # ä½¿ç”¨ TreeManager åˆ¤æ–­æ˜¯å¦å¶å­
                is_leaf = self.tree_manager.is_leaf_agent(matched_node_id)
                
                self.logger.debug(f"Match found: {matched_node_id} (Is Leaf: {is_leaf})")

                if is_leaf:
                    # æƒ…å†µ A: æ‰¾åˆ°å¶å­èŠ‚ç‚¹ -> æˆåŠŸ
                    return node_meta
                else:
                    # æƒ…å†µ B: ä¸­é—´èŠ‚ç‚¹ -> å‘ä¸‹é’»å– (Drill Down)
                    children = self.tree_manager.get_children(matched_node_id)
                    if not children:
                        break # æ­»èƒ¡åŒ
                    
                    # è§†è§’ä¸‹æ²‰ï¼šæ–°çš„çˆ¶èŠ‚ç‚¹æ˜¯åˆšæ‰åŒ¹é…åˆ°çš„èŠ‚ç‚¹
                    parent_id = matched_node_id
                    # (current_focus_node åœ¨å‘ä¸‹é’»å–æ—¶å…¶å®ä¸é‡è¦ï¼Œå› ä¸ºä¸‹ä¸€è½®ç›´æ¥å– parent çš„ children)
                    continue
            else:
                # >> æœªå‘½ä¸­åˆ†æ”¯ >>
                # æƒ…å†µ C: å½“å‰å±‚æ— åŒ¹é… -> å‘ä¸Šå›æº¯ (Bubble Up)
                if parent_id is None:
                    # å·²ç»åœ¨æ ¹å±‚ä¸”æœªå‘½ä¸­ -> æœç´¢å…¨é¢å¤±è´¥
                    self.logger.debug("Reached root layer with no match.")
                    break
                
                # ç§»åŠ¨è§†è§’å‘ä¸Šï¼š
                # æˆ‘ä»¬è¦æ‰¾ parent çš„å…„å¼Ÿï¼Œæ‰€ä»¥å°†è§†è§’èšç„¦åˆ° parent
                current_focus_node = parent_id
                # è·å– parent çš„ parent
                parent_id = self.tree_manager.get_parent(current_focus_node)
                continue
        
        return None
    
    
    def _resolve_kv_globally(self, query: str) -> Optional[Dict]:
        """
        å…¨å±€å…œåº•ï¼šåœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸­è¿›è¡Œå…³é”®è¯åŒ¹é…ï¼Œé¿å…å±‚çº§æœç´¢æ— æ³•å®šä½æ—¶ç›´æ¥å¤±è´¥ã€‚
        """
        try:
            node_service = getattr(self.tree_manager, "node_service", None)
            if not node_service:
                return None
            nodes = node_service.get_all_nodes()
            node_ids = []
            for node in nodes:
                agent_id = node.get("agent_id")
                if not agent_id:
                    continue
                if any(
                    node.get(field)
                    for field in ("database", "db", "table", "tbl", "datascope", "data_scope")
                ):
                    node_ids.append(agent_id)
            if not node_ids:
                return None
            matched_node_id = self._semantic_match_for_layer(query, node_ids)
            if not matched_node_id:
                matched_node_id = self._fallback_keyword_match(query, node_ids)
            if not matched_node_id:
                return None
            return self.tree_manager.get_agent_meta(matched_node_id)
        except Exception as e:
            self.logger.warning(f"Global fallback failed: {e}")
            return None

    def _semantic_match_for_layer(self, query: str, node_ids: List[str]) -> Optional[str]:
        """
        [é‡æ„å] ä½¿ç”¨ DashScope Qwen åˆ¤æ–­å½“å‰å±‚ä¸­å“ªä¸ªèŠ‚ç‚¹åŒ¹é… queryã€‚
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¦‚ "éœ€æŸ¥æ‰¾æ•°æ®: 'user_id', ä¸šåŠ¡æè¿°: 'å½“å‰ç™»å½•ç”¨æˆ·'"
            node_ids: å½“å‰å±‚çš„èŠ‚ç‚¹IDåˆ—è¡¨ (List[str])
        
        Returns:
            åŒ¹é…çš„ node_id (str)ï¼Œè‹¥æ— åŒ¹é…è¿”å› None
        """
        if not node_ids:
            return None

        # 1. å‡†å¤‡å€™é€‰èŠ‚ç‚¹æ•°æ®
        candidates_text = []
        valid_node_ids = [] # ç”¨äºåç»­æ ¡éªŒ LLM è¿”å›çš„ ID æ˜¯å¦åˆæ³•

        for nid in node_ids:
            # ä» TreeManager è·å–å…ƒæ•°æ®
            meta = self.tree_manager.get_agent_meta(nid)
            if not meta:
                continue

            # æå–å…³é”®ä¿¡æ¯ï¼Œæ„å»ºè¯­ä¹‰æè¿°
            # ä¼˜å…ˆå– datascopeï¼Œå…¶æ¬¡æ˜¯ capabilityï¼Œæœ€åæ˜¯ description
            ds = meta.get("datascope") or meta.get("data_scope") or "æ— æ•°æ®åŸŸå®šä¹‰"
            caps = meta.get("capability") or meta.get("capabilities") or []
            desc_text = meta.get("description", "")

            # æ ¼å¼åŒ–å„ä¸ªå­—æ®µ
            ds_str = str(ds) if isinstance(ds, (dict, list)) else str(ds)
            cap_str = ", ".join(caps) if isinstance(caps, list) else str(caps)

            # ç»„åˆæˆä¸€æ®µåˆ©äº LLM ç†è§£çš„æ–‡æœ¬
            # æ ¼å¼: [ID] æ•°æ®: ...; èƒ½åŠ›: ...; æè¿°: ...
            node_desc = (
                f"å€™é€‰èŠ‚ç‚¹ID: {nid}\n"
                f"  - æ•°æ®èŒƒå›´: {ds_str}\n"
                f"  - èƒ½åŠ›å£°æ˜: {cap_str}\n"
                f"  - èŠ‚ç‚¹æè¿°: {desc_text}"
            )
            
            candidates_text.append(node_desc)
            valid_node_ids.append(nid)

        if not candidates_text:
            return None

        candidates_block = "\n\n".join(candidates_text)

        # 2. æ„é€  Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿçš„æ•°æ®è·¯ç”±è¯­ä¹‰åŒ¹é…å¼•æ“ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®éœ€æ±‚ï¼Œä»å€™é€‰èŠ‚ç‚¹åˆ—è¡¨ä¸­é€‰æ‹©**æœ€åŒ¹é…çš„ä¸€ä¸ª**ã€‚

æ•°æ®éœ€æ±‚:
{query}

å€™é€‰èŠ‚ç‚¹åˆ—è¡¨:
---
{candidates_block}
---

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š
1. åˆ†æå“ªä¸ªèŠ‚ç‚¹çš„"æ•°æ®èŒƒå›´"æˆ–"èŠ‚ç‚¹æè¿°"èƒ½è¦†ç›–ä¸Šè¿°æ•°æ®éœ€æ±‚ã€‚
2. å¦‚æœæœ‰åŒ¹é…é¡¹ï¼Œè¯·åªè¾“å‡ºå¯¹åº”çš„ **èŠ‚ç‚¹ID** (ä¾‹å¦‚: user_agent_01)ã€‚
3. å¦‚æœæ²¡æœ‰ä¸€ä¸ªå€™é€‰èƒ½åˆç†æ»¡è¶³è¯¥éœ€æ±‚ï¼Œæˆ–è€…ç›¸å…³æ€§æä½ï¼Œè¯·åªè¾“å‡º "none"ã€‚
4. ä¸è¦è§£é‡Šï¼Œä¸è¦åŠ æ ‡ç‚¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™æ–‡å­—ã€‚
"""

        # 3. è°ƒç”¨ LLM
        try:
            # å‡è®¾ self.llm_client å·²ç»åˆå§‹åŒ–å¹¶æ³¨å…¥
            # å¦‚æœä½ ç”¨çš„æ˜¯ requests æˆ–ç‰¹å®šçš„ SDKï¼Œåœ¨è¿™é‡Œæ›¿æ¢å³å¯
            if not self.llm_client:
                self.logger.warning("LLM client missing, falling back to keyword match.")
                return self._fallback_keyword_match(query, valid_node_ids)

            # è°ƒç”¨å¤§æ¨¡å‹ (è¿™é‡Œæ¨¡æ‹Ÿä½ çš„ call_qwen é€»è¾‘)
            # answer = self.call_qwen(prompt) 
            answer = self.llm_client.generate(prompt) 
            
            # æ¸…ç†ç»“æœ
            answer = answer.strip().replace("'", "").replace('"', "").replace("`", "")
            
            self.logger.info(f"Qwen semantic match result: '{answer}' for query: '{query}'")

            # 4. ç»“æœæ ¡éªŒ
            if answer.lower() == "none":
                return None

            if answer in valid_node_ids:
                return answer
            else:
                self.logger.warning(f"Qwen returned invalid node_id: '{answer}'. Expected one of: {valid_node_ids}")
                return None

        except Exception as e:
            self.logger.error(f"Exception calling LLM/DashScope: {e}", exc_info=True)
            # é™çº§ç­–ç•¥
            return self._fallback_keyword_match(query, valid_node_ids)

    def _fallback_keyword_match(self, query: str, node_ids: List[str]) -> Optional[str]:
        """
        ç®€å•çš„å…³é”®è¯åŒ¹é…å…œåº•ç­–ç•¥
        """
        import re
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆå¿½ç•¥æ ‡ç‚¹ï¼‰
        keywords = set(re.findall(r'[\w\u4e00-\u9fa5]+', query))
        best_node = None
        max_score = 0

        for nid in node_ids:
            meta = self.tree_manager.get_agent_meta(nid) or {}
            # å°†æ‰€æœ‰å…ƒæ•°æ®è½¬ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæœç´¢
            content = (
                str(meta.get("datascope", "")) + 
                str(meta.get("description", "")) + 
                str(meta.get("capability", ""))
            ).lower()
            
            score = sum(1 for kw in keywords if kw.lower() in content)
            
            if score > max_score:
                max_score = score
                best_node = nid
        
        return best_node if max_score > 0 else None





    def enhance_param_descriptions_with_context(
        self,
        base_param_descriptions: dict,
        current_inputs: dict
        ) -> dict:
        """
        ä½¿ç”¨ LLM å°†åŸºç¡€å‚æ•°æè¿°å¢å¼ºä¸ºâ€œå¸¦ä¸Šä¸‹æ–‡â€çš„æè¿°ã€‚
        
        Args:
            base_param_descriptions: dict, e.g. {"template_id": "æµ·æŠ¥æ¨¡æ¿ID"}
            current_inputs: dict, e.g. {"tenant_id": "t_abc", "activity_id": "act_123"}
        
        Returns:
            dict: {"template_id": "æµ·æŠ¥æ¨¡æ¿IDï¼Œå±äºç§Ÿæˆ· t_abc å’Œæ´»åŠ¨ act_123"}
        """
        if not base_param_descriptions:
            return {}
        
        if not self.tree_manager or not self.llm_client:
            self.set_dependencies()

        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆåªä¿ç•™éç©ºã€éæ•æ„Ÿå­—æ®µï¼Œå¯æ‰©å±•è¿‡æ»¤é€»è¾‘ï¼‰
        context_items = []
        for k, v in current_inputs.items():
                
            # 2. ç±»å‹å®‰å…¨æ£€æŸ¥
            if not v or not isinstance(v, (str, int, float, bool)):
                continue
                
            v_str = str(v)
            
            # 3. æ”¾å®½é•¿åº¦é™åˆ¶ï¼šå»ºè®®ä» 100 æå‡åˆ° 500 æˆ– 1000
            # è¿™æ ·æ—¢èƒ½é˜²ä½å‡ ä¸‡å­—çš„è¶…å¤§æ–‡æœ¬ï¼Œåˆèƒ½å®¹çº³ URL å’Œ ä¸šåŠ¡æè¿°
            if len(v_str) < 1000:  
                context_items.append(f"{k}: {v_str}")
            else:
                # å¯é€‰ï¼šå¯¹äºè¶…é•¿æ–‡æœ¬ï¼Œæˆªå–å‰ 100 ä¸ªå­—ç¬¦ä½œä¸ºâ€œæ‘˜è¦â€æ”¾è¿›å»
                # è¿™æ · LLM è‡³å°‘çŸ¥é“æœ‰è¿™ä¸ªå­—æ®µå­˜åœ¨
                context_items.append(f"{k}: {v_str[:100]}... (content too long)")
        
        context_str = "\n".join(context_items) if context_items else "æ— å¯ç”¨ä¸Šä¸‹æ–‡"

        # æ„å»ºå‚æ•°åˆ—è¡¨å­—ç¬¦ä¸²
        params_list = "\n".join([
            f"- {name}: {desc}" 
            for name, desc in base_param_descriptions.items()
        ])

        # === æ„å»º LLM Prompt ===
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å‚æ•°æè¿°å¢å¼ºå™¨ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªå‚æ•°ç”Ÿæˆå¢å¼ºç‰ˆçš„ä¸­æ–‡æè¿°ã€‚

    è¦æ±‚ï¼š
    - è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„ JSON æ ¼å¼ï¼š{{ "å‚æ•°å": "å¢å¼ºåçš„æè¿°" }}
    - åœ¨åŸå§‹æè¿°åŸºç¡€ä¸Šï¼Œ**è‡ªç„¶èå…¥æ‰€æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼ˆå¦‚ tenant_idã€activity_id ç­‰ï¼‰
    - ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºå¸®åŠ©åç»­ç³»ç»Ÿç²¾å‡†æŸ¥è¯¢è¯¥å‚æ•°å€¼ï¼Œè¯·æ˜ç¡®å†™å‡ºå½’å±ï¼ˆä¾‹å¦‚ï¼šâ€œå±äºç§Ÿæˆ·xxxx çš„æ´»åŠ¨ xxxxâ€ï¼‰
    - å¦‚æœæŸä¸ªä¸Šä¸‹æ–‡ä¸å‚æ•°æ˜æ˜¾æ— å…³ï¼Œå¯ä¸å¼ºè¡ŒåŠ å…¥
    - æè¿°è¦ç®€æ´ã€ä¸“ä¸šã€å¯è¢«è‡ªåŠ¨åŒ–ç³»ç»Ÿç†è§£
    - **ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¸Šä¸‹æ–‡**
    - **ä¸è¦æ”¹å˜å‚æ•°å**
    - åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—

    ã€å¯ç”¨ä¸Šä¸‹æ–‡ã€‘
    {context_str}

    ã€å¾…å¢å¼ºçš„å‚æ•°åŠåŸºç¡€æè¿°ã€‘
    {params_list}
    """

        # === è°ƒç”¨ LLM ===
        try:
            response = self.llm_client.generate(
                prompt=prompt,
                parse_json=True,
            )
            result = response


            # ä¿è¯è¾“å‡º key ä¸è¾“å…¥ä¸€è‡´ï¼ˆé˜²æ­¢ LLM æ”¹åï¼‰
            aligned_result = {}
            for param_name in base_param_descriptions:
                if param_name in result:
                    aligned_result[param_name] = str(result[param_name]).strip()
                else:
                    # å›é€€ï¼šç”¨åŸå§‹æè¿° + ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                    fallback_desc = base_param_descriptions[param_name]
                    if context_items:
                        fallback_desc += "ï¼Œä¸Šä¸‹æ–‡ï¼š" + "ï¼›".join(context_items)
                    aligned_result[param_name] = fallback_desc

            return aligned_result

        except Exception as e:
            print(f"[WARN] LLM å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥: {e}")
            # å…¨éƒ¨å›é€€åˆ°åŸºç¡€æè¿° + ä¸Šä¸‹æ–‡æ‹¼æ¥
            fallback = {}
            context_suffix = "ï¼ˆä¸Šä¸‹æ–‡ï¼š" + "ï¼›".join(context_items) + "ï¼‰" if context_items else ""
            for name, desc in base_param_descriptions.items():
                fallback[name] = desc + context_suffix
            return fallback



    def pre_fill_known_params_with_llm(
        self,
        base_param_descriptions: dict,
        current_context_str: str
    ) -> tuple[dict, dict]:
        """
        ä½¿ç”¨ LLM ä»è‡ªç”±æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­æå–å¯è¯†åˆ«çš„å‚æ•°å€¼ã€‚

        Args:
            base_param_descriptions: {"user_id": "ç”¨æˆ·ID", "tenant_id": "ç§Ÿæˆ·ID", ...}
            current_context_str: ä»»æ„ä¸Šä¸‹æ–‡ï¼Œå¦‚ "å½“å‰ç”¨æˆ·æ˜¯ test_admin_001ï¼Œå±äºç§Ÿæˆ· test_tenant_001"

        Returns:
            (filled_values, remaining_params)
        """
        if not base_param_descriptions:
            return {}, {}

        if not self.tree_manager or not self.llm_client:
            self.set_dependencies()

        filled = {}

        # === é¢„å¤„ç†ï¼šè§£æç‰¹æ®Šæ ¼å¼çš„ user_id ===
        # æ ¼å¼å¦‚: <user_id:1,tenant_id:1> æˆ– <user_id:1>,<tenant_id:1>
        parsed_ids = self._parse_user_id_format(current_context_str)
        if parsed_ids:
            for param_name in base_param_descriptions:
                if param_name in parsed_ids:
                    filled[param_name] = parsed_ids[param_name]
                    logger.info(f"Pre-filled '{param_name}' from special format: {parsed_ids[param_name]}")

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å‰©ä½™å‚æ•°éœ€è¦ LLM å¤„ç†
        remaining_for_llm = {
            k: v for k, v in base_param_descriptions.items()
            if k not in filled
        }

        if not remaining_for_llm:
            return filled, {}

        # æ„å»ºå‚æ•°è¯´æ˜
        params_info = "\n".join([
            f"- {name}: {desc}"
            for name, desc in remaining_for_llm.items()
        ])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‚æ•°å€¼æå–å™¨ã€‚è¯·ä»ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¸­ï¼Œå°½å¯èƒ½æå–å‡ºä¸ç›®æ ‡å‚æ•°åŒ¹é…çš„å…·ä½“å€¼ã€‚

    è¦æ±‚ï¼š
    - åªæå–æ˜ç¡®æåŠæˆ–å¯åˆç†æ¨æ–­çš„å€¼ï¼›
    - å¦‚æœæŸä¸ªå‚æ•°æ— æ³•ç¡®å®šï¼Œä¸è¦çŒœæµ‹ï¼Œç›´æ¥è·³è¿‡ï¼›
    - è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼ JSON æ ¼å¼ï¼š{{ "å‚æ•°å": "æå–çš„å€¼" }}
    - å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼›
    - ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—ï¼ŒåŒ…æ‹¬è§£é‡Šã€markdownã€å‰ç¼€ã€‚

    ã€ç›®æ ‡å‚æ•°å®šä¹‰ã€‘
    {params_info}

    ã€å½“å‰ä¸Šä¸‹æ–‡ã€‘
    {current_context_str}
    """

        try:
            response = self.llm_client.generate(
                prompt=prompt,
                parse_json=True,
            )
            # text = response.output.text.strip()

            # æå– JSON
            # json_match = re.search(r"\{.*\}", text, re.DOTALL)
            json_match = response
            if json_match:
                # extracted = json.loads(json_match.group(0))
                extracted = json_match
                # åªä¿ç•™åˆæ³•å‚æ•°å + å­—ç¬¦ä¸²å€¼
                for k, v in extracted.items():
                    if k in remaining_for_llm and isinstance(v, str) and v.strip():
                        filled[k] = v.strip()
        except Exception as e:
            print(f"[WARN] LLM é¢„å¡«å……å¤±è´¥ï¼Œè·³è¿‡: {e}")

        # åˆ†ç¦»å·²å¡«å……å’Œå‰©ä½™å‚æ•°
        remaining = {
            k: v for k, v in base_param_descriptions.items()
            if k not in filled
        }

        return filled, remaining

    def _parse_user_id_format(self, context: any) -> dict:
        """
        è§£æç‰¹æ®Šæ ¼å¼çš„ user_id å­—ç¬¦ä¸²

        æ”¯æŒæ ¼å¼:
        - <user_id:1,tenant_id:1>
        - <user_id:1>,<tenant_id:1>
        - åµŒå¥—åœ¨å­—å…¸ä¸­çš„ '_user_id': '<user_id:1,tenant_id:1>'

        Args:
            context: ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸

        Returns:
            dict: è§£æå‡ºçš„å‚æ•°ï¼Œå¦‚ {"user_id": "1", "tenant_id": "1"}
        """
        result = {}

        # å°† context è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæœç´¢
        # ä½¿ç”¨å®‰å…¨çš„åºåˆ—åŒ–æ–¹æ³•ï¼Œå¤„ç† ContextEntry ç­‰ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        context_str = self._safe_serialize_for_parsing(context)

        # åŒ¹é…æ¨¡å¼: <key:value> æˆ– <key:value,key2:value2>
        # æ¨¡å¼1: <user_id:1,tenant_id:1>
        pattern1 = r'<([a-zA-Z_]+):([^,>]+)(?:,([a-zA-Z_]+):([^>]+))?>'
        matches = re.findall(pattern1, context_str)

        for match in matches:
            if match[0] and match[1]:
                result[match[0]] = match[1].strip()
            if match[2] and match[3]:
                result[match[2]] = match[3].strip()

        # æ¨¡å¼2: å•ç‹¬çš„ <key:value>
        pattern2 = r'<([a-zA-Z_]+):([^<>]+)>'
        matches2 = re.findall(pattern2, context_str)
        for key, value in matches2:
            if key not in result:
                result[key] = value.strip()

        return result

    def _safe_serialize_for_parsing(self, obj: any) -> str:
        """
        å®‰å…¨åœ°å°†å¯¹è±¡åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæ­£åˆ™åŒ¹é…è§£æ

        å¤„ç† ContextEntryã€Pydantic BaseModel ç­‰ä¸å¯ç›´æ¥ JSON åºåˆ—åŒ–çš„å¯¹è±¡

        Args:
            obj: ä»»æ„å¯¹è±¡

        Returns:
            str: åºåˆ—åŒ–åçš„å­—ç¬¦ä¸²
        """
        from pydantic import BaseModel

        def make_serializable(item: any) -> any:
            if item is None:
                return None
            if isinstance(item, (str, int, float, bool)):
                return item
            if isinstance(item, BaseModel):
                return item.model_dump()
            if isinstance(item, dict):
                return {k: make_serializable(v) for k, v in item.items()}
            if isinstance(item, (list, tuple)):
                return [make_serializable(i) for i in item]
            # å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
            try:
                return str(item)
            except Exception:
                return repr(item)

        try:
            serializable = make_serializable(obj)
            return json.dumps(serializable, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"Failed to serialize context for parsing: {e}")
            return str(obj)
    



    # ----------------------------------------------------------
    # è¾…åŠ©åŠŸèƒ½
    # ----------------------------------------------------------

    def extract_context(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿ç•™åŸæœ‰åŸºç¡€æå–é€»è¾‘"""
        base_ctx = {}
        fields = ['task_id', 'task_type', 'user_id', 'content', 'query', 'payload']
        for f in fields:
            if f in task_data:
                base_ctx[f] = task_data[f]
        return base_ctx

    def register_context_template(self, name: str, template: Dict) -> None:
        self.context_templates[name] = template

    def enrich_context_from_result(
        self, 
        msg: 'TaskMessage', 
        result: Any, 
        task_name: str = ""
    ) -> None:
        """
        ä»ä»»åŠ¡æ‰§è¡Œç»“æœä¸­å¯Œé›†ä¸Šä¸‹æ–‡
        
        Args:
            msg: TaskMessage å¯¹è±¡ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            result: ä»»åŠ¡æ‰§è¡Œç»“æœ
            task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼‰
        """
        # ç¤ºä¾‹ï¼šä» result ä¸­æå–ç»“æ„åŒ–å­—æ®µ
        if isinstance(result, dict):
            for key, value in result.items():
                # è‡ªå®šä¹‰è¿‡æ»¤é€»è¾‘ï¼šåªä¿ç•™åŸºæœ¬ç±»å‹å’Œéç©ºå€¼
                if value is not None and isinstance(value, (str, int, float, bool, list, dict)):
                    # ç”Ÿæˆå®‰å…¨é”®åï¼ŒåŒ…å«ä»»åŠ¡è·¯å¾„å‰ç¼€
                    safe_key = f"{msg.task_path.replace('/', '_')}.{key}"
                    msg.enriched_context[safe_key] = value
        elif isinstance(result, (list, tuple)):
            # å¤„ç†åˆ—è¡¨ç»“æœï¼Œæ·»åŠ ç´¢å¼•
            for i, item in enumerate(result[:10]):  # æœ€å¤šå–å‰10ä¸ªå…ƒç´ 
                if isinstance(item, dict):
                    for key, value in item.items():
                        if value is not None and isinstance(value, (str, int, float, bool)):
                            safe_key = f"{msg.task_path.replace('/', '_')}.item_{i}.{key}"
                            msg.enriched_context[safe_key] = value
        elif isinstance(result, (str, int, float, bool)):
            # å¤„ç†å•ä¸ªåŸºæœ¬ç±»å‹ç»“æœ
            safe_key = f"{msg.task_path.replace('/', '_')}.result"
            msg.enriched_context[safe_key] = result

    def extract_params_for_capability(
        self, 
        capability: str, 
        enriched_context: Dict[str, Any], 
        global_context: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], List[str]]:
        """
        ä¸ºç‰¹å®šèƒ½åŠ›æ™ºèƒ½æå–å‚æ•°
        
        Args:
            capability: èƒ½åŠ›åç§°
            enriched_context: å¯Œä¸Šä¸‹æ–‡
            global_context: å…¨å±€ä¸Šä¸‹æ–‡
            
        Returns:
            (å¯ç”¨å‚æ•°, ç¼ºå¤±å‚æ•°åˆ—è¡¨)
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ CAPABILITY_SPECS è¿›è¡Œå®ç°
        # ç¤ºä¾‹å®ç°ï¼šåŸºäºç®€å•çš„å‚æ•°æ˜ å°„
        spec = self._get_capability_spec(capability)
        if not spec:
            return {}, []
            
        params = {}
        missing = []

        for param_name, config in spec["parameters"].items():
            found = False

            # 1. ä¼˜å…ˆä» enriched_context åŒ¹é…ï¼ˆæ”¯æŒåˆ«åï¼‰
            for alias in [param_name] + config.get("aliases", []):
                if alias in enriched_context:
                    params[param_name] = enriched_context[alias]
                    found = True
                    break

            # 2. å°è¯•ä» global_context è·å–ï¼ˆå¦‚ user_idï¼‰
            if not found and param_name in global_context:
                params[param_name] = global_context[param_name]
                found = True

            # 3. ä»ç¼ºå¤±ï¼Ÿ
            if not found:
                missing.append(param_name)

        return params, missing
    
    def _get_capability_spec(self, capability: str) -> Optional[Dict[str, Any]]:
        """
        è·å–èƒ½åŠ›çš„å‚æ•°è§„æ ¼
        
        Args:
            capability: èƒ½åŠ›åç§°
            
        Returns:
            èƒ½åŠ›è§„æ ¼å­—å…¸ï¼ŒåŒ…å« parameters å­—æ®µ
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç³»ç»Ÿä¸­çš„èƒ½åŠ›è§„æ ¼è¿›è¡Œå®ç°
        # ç¤ºä¾‹ï¼šè¿”å›ä¸€ä¸ªç®€å•çš„é»˜è®¤è§„æ ¼
        return {
            "parameters": {
                # é»˜è®¤å‚æ•°è§„æ ¼ï¼Œå®é™…ç³»ç»Ÿä¸­åº”è¯¥ä»é…ç½®æˆ–æ³¨å†Œä¸­å¿ƒè·å–
                "query": {"aliases": ["q", "question", "prompt"]},
                "user_id": {"aliases": ["uid", "user"]},
                "tenant_id": {"aliases": ["tid", "tenant"]}
            }
        }
    
    # ----------------------------------------------------------
    # è¯­ä¹‰æŒ‡é’ˆè¡¥å…¨ï¼šæ¶ˆè§£ä»£è¯æ­§ä¹‰
    # ----------------------------------------------------------

    # å¸¸è§çš„æ¨¡ç³Šå¼•ç”¨æ¨¡å¼
    AMBIGUOUS_PATTERNS = [
        # ä»£è¯
        r'\b(ä»–|å¥¹|å®ƒ|ä»–ä»¬|å¥¹ä»¬|å®ƒä»¬)\b',
        r'\b(è¿™ä¸ª|é‚£ä¸ª|è¿™äº›|é‚£äº›|è¯¥|æ­¤|å…¶)\b',
        # æŒ‡ç¤ºæ€§å¼•ç”¨
        r'\b(ä¸Šè¿°|å‰è¿°|æ‰€è¿°|ä¸Šé¢çš„|ä¹‹å‰çš„|åˆšæ‰çš„)\b',
        r'\b(è¯¥ç”¨æˆ·|è¯¥å®¢æˆ·|è¯¥è®¢å•|è¯¥å•†å“|è¯¥è®°å½•)\b',
        r'\b(å½“å‰ç”¨æˆ·|å½“å‰å®¢æˆ·|å½“å‰è®¢å•)\b',
        # è‹±æ–‡ä»£è¯
        r'\b(this|that|these|those|the)\s+(user|customer|order|item|record)\b',
        r'\b(he|she|it|they|him|her|them)\b',
    ]

    def resolve_semantic_pointers(
        self,
        param_descriptions: Dict[str, str],
        current_context: Dict[str, Any],
        agent_id: str,
        user_id: str,
        max_ancestor_levels: int = 3
    ) -> Dict[str, Dict[str, Any]]:
        """
        å°†æ¨¡ç³Šçš„å‚æ•°æè¿°è½¬åŒ–ä¸ºè¯­ä¹‰æŒ‡é’ˆï¼Œæ¶ˆè§£ä»£è¯æ­§ä¹‰ã€‚

        æ ¸å¿ƒæœºåˆ¶ï¼š
        1. æ£€æµ‹å‚æ•°æè¿°ä¸­çš„æ¨¡ç³Šå¼•ç”¨ï¼ˆä»£è¯ã€æŒ‡ç¤ºè¯ï¼‰
        2. æ²¿æ ‘å‘ä¸Šå›æº¯çˆ¶çº§ Agent çš„ä¸šåŠ¡è®°å¿†
        3. ä½¿ç”¨ LLM å°†å±€éƒ¨æ„å›¾ä¸çˆ¶çº§è®°å¿†è¿›è¡Œè¯­ä¹‰å¯¹é½
        4. ç”Ÿæˆè‡ªåŒ…å«çš„è¯­ä¹‰æŒ‡é’ˆ

        Args:
            param_descriptions: å‚æ•°å -> åŸå§‹æè¿°ï¼Œå¦‚ {"client_id": "è¯¥ç”¨æˆ·çš„ID"}
            current_context: å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŒ…å« content, description, global_context, enriched_context
            agent_id: å½“å‰ Agent ID
            user_id: ç”¨æˆ· ID
            max_ancestor_levels: æœ€å¤§å›æº¯å±‚æ•°

        Returns:
            Dict[str, Dict]: å‚æ•°å -> è¯­ä¹‰æŒ‡é’ˆä¿¡æ¯
            {
                "client_id": {
                    "original_desc": "è¯¥ç”¨æˆ·çš„ID",
                    "resolved_desc": "æ˜¨å¤©ç¬¬äºŒä¸ªéœ€è¦é€€æ¬¾èµ„æ ¼æ£€æŸ¥çš„å®¢æˆ·çš„ID",
                    "confidence": 0.9,
                    "resolution_chain": ["çˆ¶çº§ä»»åŠ¡ç›®æ ‡ï¼šå¤„ç†æ˜¨å¤©çš„ç¬¬äºŒä¸ªå®¢æˆ·çš„æŠ•è¯‰"],
                    "has_ambiguity": True
                }
            }
        """
        if not param_descriptions:
            return {}

        if not self.tree_manager or not self.llm_client:
            self.set_dependencies()

        result = {}

        # 1. æ£€æµ‹å“ªäº›å‚æ•°åŒ…å«æ¨¡ç³Šå¼•ç”¨
        params_with_ambiguity = {}
        params_without_ambiguity = {}

        for param_name, desc in param_descriptions.items():
            if self._detect_ambiguous_references(desc):
                params_with_ambiguity[param_name] = desc
            else:
                params_without_ambiguity[param_name] = desc

        # 2. å¯¹äºæ²¡æœ‰æ¨¡ç³Šå¼•ç”¨çš„å‚æ•°ï¼Œç›´æ¥è¿”å›åŸå§‹æè¿°
        for param_name, desc in params_without_ambiguity.items():
            result[param_name] = {
                "original_desc": desc,
                "resolved_desc": desc,
                "confidence": 1.0,
                "resolution_chain": [],
                "has_ambiguity": False
            }

        # 3. å¦‚æœæ²¡æœ‰éœ€è¦è§£æçš„å‚æ•°ï¼Œç›´æ¥è¿”å›
        if not params_with_ambiguity:
            return result

        # 4. è·å–çˆ¶çº§ä¸Šä¸‹æ–‡
        ancestor_context_summary = ""
        try:
            from ..llm_memory.unified_memory import UnifiedMemory
            from ..llm_memory.interface import IMemoryCapability
            from .. import get_capability

            memory_cap = get_capability("llm_memory", expected_type=IMemoryCapability)
            if hasattr(memory_cap, '_memory_manager') and memory_cap._memory_manager:
                ancestor_context_summary = memory_cap._memory_manager.build_ancestor_context_summary(
                    user_id=user_id,
                    agent_id=agent_id,
                    tree_manager=self.tree_manager,
                    max_levels=max_ancestor_levels
                )
        except Exception as e:
            self.logger.warning(f"Failed to get ancestor context: {e}")

        # 5. å¦‚æœæ²¡æœ‰çˆ¶çº§ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»å½“å‰ä¸Šä¸‹æ–‡ä¸­è§£æ
        if not ancestor_context_summary:
            # ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡è¿›è¡Œå¢å¼º
            for param_name, desc in params_with_ambiguity.items():
                enhanced = self._enhance_with_current_context(
                    param_name, desc, current_context
                )
                result[param_name] = enhanced
            return result

        # 6. ä½¿ç”¨ LLM æ‰¹é‡è§£ææ¨¡ç³Šå¼•ç”¨
        resolved = self._batch_resolve_ambiguities(
            params_with_ambiguity,
            current_context,
            ancestor_context_summary
        )

        result.update(resolved)
        return result

    def _detect_ambiguous_references(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ¨¡ç³Šå¼•ç”¨ï¼ˆä»£è¯ã€æŒ‡ç¤ºè¯ç­‰ï¼‰

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦åŒ…å«æ¨¡ç³Šå¼•ç”¨
        """
        if not text:
            return False

        text_lower = text.lower()

        for pattern in self.AMBIGUOUS_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                return True

        return False

    def _enhance_with_current_context(
        self,
        param_name: str,
        original_desc: str,
        current_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡å¢å¼ºå‚æ•°æè¿°ï¼ˆæ— çˆ¶çº§ä¸Šä¸‹æ–‡æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰

        Args:
            param_name: å‚æ•°å
            original_desc: åŸå§‹æè¿°
            current_context: å½“å‰ä¸Šä¸‹æ–‡

        Returns:
            Dict: è¯­ä¹‰æŒ‡é’ˆä¿¡æ¯
        """
        # ä»å½“å‰ä¸Šä¸‹æ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯
        context_parts = []

        content = current_context.get("content", "")
        description = current_context.get("description", "")
        global_ctx = current_context.get("global_context", {})
        enriched_ctx = current_context.get("enriched_context", {})

        if content:
            context_parts.append(f"ä»»åŠ¡å†…å®¹: {content[:200]}")
        if description:
            context_parts.append(f"ä»»åŠ¡æè¿°: {description[:200]}")

        # ä» global_context å’Œ enriched_context ä¸­æå–ç›¸å…³å­—æ®µ
        for ctx in [global_ctx, enriched_ctx]:
            if isinstance(ctx, dict):
                for k, v in ctx.items():
                    if isinstance(v, (str, int, float)) and v:
                        # æ£€æŸ¥æ˜¯å¦ä¸å‚æ•°åç›¸å…³
                        if param_name.lower() in k.lower() or k.lower() in param_name.lower():
                            context_parts.append(f"{k}: {v}")

        if context_parts:
            enhanced_desc = f"{original_desc}ï¼ˆä¸Šä¸‹æ–‡ï¼š{'; '.join(context_parts[:3])}ï¼‰"
        else:
            enhanced_desc = original_desc

        return {
            "original_desc": original_desc,
            "resolved_desc": enhanced_desc,
            "confidence": 0.5,  # è¾ƒä½ç½®ä¿¡åº¦ï¼Œå› ä¸ºæ²¡æœ‰çˆ¶çº§ä¸Šä¸‹æ–‡
            "resolution_chain": context_parts[:3],
            "has_ambiguity": True
        }

    def _batch_resolve_ambiguities(
        self,
        params_with_ambiguity: Dict[str, str],
        current_context: Dict[str, Any],
        ancestor_context_summary: str
    ) -> Dict[str, Dict[str, Any]]:
        """
        ä½¿ç”¨ LLM æ‰¹é‡è§£ææ¨¡ç³Šå¼•ç”¨

        Args:
            params_with_ambiguity: åŒ…å«æ¨¡ç³Šå¼•ç”¨çš„å‚æ•°
            current_context: å½“å‰ä¸Šä¸‹æ–‡
            ancestor_context_summary: çˆ¶çº§ä¸Šä¸‹æ–‡æ‘˜è¦

        Returns:
            Dict: å‚æ•°å -> è¯­ä¹‰æŒ‡é’ˆä¿¡æ¯
        """
        if not self.llm_client:
            # é™çº§ï¼šè¿”å›åŸå§‹æè¿°
            return {
                param_name: {
                    "original_desc": desc,
                    "resolved_desc": desc,
                    "confidence": 0.3,
                    "resolution_chain": [],
                    "has_ambiguity": True
                }
                for param_name, desc in params_with_ambiguity.items()
            }

        # æ„å»ºå½“å‰ä¸Šä¸‹æ–‡æ‘˜è¦
        current_context_str = ""
        content = current_context.get("content", "")
        description = current_context.get("description", "")
        if content:
            current_context_str += f"ä»»åŠ¡å†…å®¹: {content[:300]}\n"
        if description:
            current_context_str += f"ä»»åŠ¡æè¿°: {description[:300]}\n"

        # æ„å»ºå‚æ•°åˆ—è¡¨
        params_list = "\n".join([
            f"- {name}: {desc}"
            for name, desc in params_with_ambiguity.items()
        ])

        # æ„å»º Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰æ¶ˆæ­§åŠ©æ‰‹ã€‚è¯·æ ¹æ®çˆ¶çº§ä¸šåŠ¡ä¸Šä¸‹æ–‡ï¼Œå°†æ¨¡ç³Šçš„å‚æ•°æè¿°è½¬åŒ–ä¸ºç²¾ç¡®çš„è¯­ä¹‰æè¿°ã€‚

ã€å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ã€‘
{current_context_str}

ã€çˆ¶çº§ä¸šåŠ¡è®°å¿†ã€‘ï¼ˆä»è¿‘åˆ°è¿œï¼‰
{ancestor_context_summary}

ã€å¾…è§£æçš„å‚æ•°ã€‘ï¼ˆåŒ…å«æ¨¡ç³Šå¼•ç”¨å¦‚"è¯¥ç”¨æˆ·"ã€"ä»–"ã€"è¿™ä¸ª"ç­‰ï¼‰
{params_list}

ã€ä»»åŠ¡ã€‘
1. åˆ†ææ¯ä¸ªå‚æ•°æè¿°ä¸­çš„æ¨¡ç³Šå¼•ç”¨
2. ä»çˆ¶çº§è®°å¿†ä¸­æ‰¾åˆ°å¯¹åº”çš„ç²¾ç¡®ä¿¡æ¯
3. ç”Ÿæˆè‡ªåŒ…å«çš„è¯­ä¹‰æè¿°ï¼Œä½¿å¾—ä»…å‡­æ­¤æè¿°å°±èƒ½ç²¾ç¡®å®šä½æ•°æ®

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¾“å‡º JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "å‚æ•°å1": {{
        "resolved_desc": "å®Œæ•´çš„è¯­ä¹‰æè¿°",
        "confidence": 0.0-1.0,
        "resolution_chain": ["ä»çˆ¶çº§è·å–çš„å…³é”®ä¿¡æ¯1", "å…³é”®ä¿¡æ¯2"]
    }},
    "å‚æ•°å2": {{
        ...
    }}
}}

ã€ç¤ºä¾‹ã€‘
è¾“å…¥å‚æ•°: client_id: "è¯¥ç”¨æˆ·çš„ID"
çˆ¶çº§è®°å¿†: "ä»»åŠ¡ç›®æ ‡ï¼šå¤„ç†æ˜¨å¤©çš„ç¬¬äºŒä¸ªå®¢æˆ·çš„æŠ•è¯‰"
è¾“å‡º:
{{
    "client_id": {{
        "resolved_desc": "æ˜¨å¤©ç¬¬äºŒä¸ªéœ€è¦å¤„ç†æŠ•è¯‰çš„å®¢æˆ·çš„ID",
        "confidence": 0.9,
        "resolution_chain": ["çˆ¶çº§ä»»åŠ¡ç›®æ ‡ï¼šå¤„ç†æ˜¨å¤©çš„ç¬¬äºŒä¸ªå®¢æˆ·çš„æŠ•è¯‰"]
    }}
}}

æ³¨æ„ï¼š
- å¦‚æœæ— æ³•ä»çˆ¶çº§è®°å¿†ä¸­æ‰¾åˆ°å¯¹åº”ä¿¡æ¯ï¼Œconfidence è®¾ä¸º 0.3-0.5
- resolved_desc å¿…é¡»æ˜¯è‡ªåŒ…å«çš„ï¼Œä¸èƒ½åŒ…å«ä»£è¯
- åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Š
"""

        try:
            response = self.llm_client.generate(
                prompt=prompt,
                parse_json=True,
                max_tokens=1024
            )

            result = {}
            for param_name, original_desc in params_with_ambiguity.items():
                if param_name in response:
                    resolved_info = response[param_name]
                    result[param_name] = {
                        "original_desc": original_desc,
                        "resolved_desc": resolved_info.get("resolved_desc", original_desc),
                        "confidence": float(resolved_info.get("confidence", 0.5)),
                        "resolution_chain": resolved_info.get("resolution_chain", []),
                        "has_ambiguity": True
                    }
                else:
                    # LLM æœªè¿”å›è¯¥å‚æ•°ï¼Œä½¿ç”¨åŸå§‹æè¿°
                    result[param_name] = {
                        "original_desc": original_desc,
                        "resolved_desc": original_desc,
                        "confidence": 0.3,
                        "resolution_chain": [],
                        "has_ambiguity": True
                    }

            return result

        except Exception as e:
            self.logger.error(f"LLM batch resolve failed: {e}")
            # é™çº§ï¼šè¿”å›åŸå§‹æè¿°
            return {
                param_name: {
                    "original_desc": desc,
                    "resolved_desc": desc,
                    "confidence": 0.3,
                    "resolution_chain": [],
                    "has_ambiguity": True
                }
                for param_name, desc in params_with_ambiguity.items()
            }

    def enhance_param_descriptions_with_semantic_pointers(
        self,
        base_param_descriptions: Dict[str, str],
        current_context: Dict[str, Any],
        agent_id: str,
        user_id: str
    ) -> Dict[str, str]:
        """
        å¢å¼ºç‰ˆå‚æ•°æè¿°ï¼šç»“åˆè¯­ä¹‰æŒ‡é’ˆè¡¥å…¨ã€‚

        è¿™æ˜¯ enhance_param_descriptions_with_context çš„å¢å¼ºç‰ˆæœ¬ï¼Œ
        ä¼šå…ˆè¿›è¡Œè¯­ä¹‰æŒ‡é’ˆè¡¥å…¨ï¼Œå†è¿›è¡Œä¸Šä¸‹æ–‡å¢å¼ºã€‚

        Args:
            base_param_descriptions: åŸºç¡€å‚æ•°æè¿°
            current_context: å½“å‰ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« content, description, global_context, enriched_contextï¼‰
            agent_id: å½“å‰ Agent ID
            user_id: ç”¨æˆ· ID

        Returns:
            Dict[str, str]: å¢å¼ºåçš„å‚æ•°æè¿°
        """
        if not base_param_descriptions:
            return {}

        # 1. å…ˆè¿›è¡Œè¯­ä¹‰æŒ‡é’ˆè¡¥å…¨
        semantic_pointers = self.resolve_semantic_pointers(
            param_descriptions=base_param_descriptions,
            current_context=current_context,
            agent_id=agent_id,
            user_id=user_id
        )

        # 2. æå–è¡¥å…¨åçš„æè¿°
        enhanced_descriptions = {}
        for param_name, pointer_info in semantic_pointers.items():
            # ä½¿ç”¨è¡¥å…¨åçš„æè¿°
            resolved_desc = pointer_info.get("resolved_desc", base_param_descriptions.get(param_name, ""))
            confidence = pointer_info.get("confidence", 1.0)

            # å¦‚æœç½®ä¿¡åº¦è¾ƒä½ï¼Œä¿ç•™åŸå§‹æè¿°ä½œä¸ºå¤‡æ³¨
            if confidence < 0.6 and pointer_info.get("has_ambiguity"):
                original = pointer_info.get("original_desc", "")
                if original and original != resolved_desc:
                    resolved_desc = f"{resolved_desc}ï¼ˆåŸå§‹æè¿°ï¼š{original}ï¼‰"

            enhanced_descriptions[param_name] = resolved_desc

        # 3. å†è¿›è¡Œå¸¸è§„çš„ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆä½¿ç”¨ enriched_context ä¸­çš„å…·ä½“å€¼ï¼‰
        current_inputs = {}
        enriched_ctx = current_context.get("enriched_context", {})
        global_ctx = current_context.get("global_context", {})

        if isinstance(enriched_ctx, dict):
            current_inputs.update(enriched_ctx)
        if isinstance(global_ctx, dict):
            current_inputs.update(global_ctx)

        if current_inputs:
            enhanced_descriptions = self.enhance_param_descriptions_with_context(
                enhanced_descriptions,
                current_inputs
            )

        return enhanced_descriptions

    # ----------------------------------------------------------
    # Schema æ‘˜è¦ + æŒ‰éœ€å±•å¼€ï¼šç»Ÿä¸€å‚æ•°è§£æ
    # ----------------------------------------------------------

    def build_schema_summary(self, data: Any) -> Any:
        """
        ä»å®é™…æ•°æ®è‡ªåŠ¨ç”Ÿæˆ Schema æ‘˜è¦ï¼ˆç±»å‹ä¿¡æ¯ï¼‰

        ç”¨äºè®© LLM å¿«é€Ÿäº†è§£æ•°æ®ç»“æ„ï¼Œè€Œä¸éœ€è¦çœ‹å®Œæ•´å†…å®¹ã€‚

        Args:
            data: ä»»æ„æ•°æ®

        Returns:
            Schema æ‘˜è¦ï¼Œä¿ç•™ç»“æ„ä½†ç”¨ç±»å‹æ›¿ä»£å€¼

        Example:
            è¾“å…¥: {"user_id": "123", "profile": {"name": "å¼ ä¸‰", "age": 25}}
            è¾“å‡º: {"user_id": "string", "profile": {"name": "string", "age": "int"}}
        """
        if data is None:
            return "null"

        if isinstance(data, str):
            # å¯¹äºè¾ƒé•¿çš„å­—ç¬¦ä¸²ï¼Œæ˜¾ç¤ºé•¿åº¦ä¿¡æ¯
            if len(data) > 100:
                return f"string(len={len(data)})"
            return "string"

        if isinstance(data, bool):
            return "bool"

        if isinstance(data, int):
            return "int"

        if isinstance(data, float):
            return "float"

        if isinstance(data, dict):
            if not data:
                return "{}"
            # é€’å½’å¤„ç†å­—å…¸çš„æ¯ä¸ªå­—æ®µ
            return {k: self.build_schema_summary(v) for k, v in data.items()}

        if isinstance(data, (list, tuple)):
            if not data:
                return "list[]"
            # å–ç¬¬ä¸€ä¸ªå…ƒç´ çš„ schema ä½œä¸ºåˆ—è¡¨å…ƒç´ ç±»å‹
            first_schema = self.build_schema_summary(data[0])
            if len(data) > 1:
                return f"list[{first_schema}](len={len(data)})"
            return f"list[{first_schema}]"

        # Pydantic BaseModel
        try:
            from pydantic import BaseModel
            if isinstance(data, BaseModel):
                return self.build_schema_summary(data.model_dump())
        except ImportError:
            pass

        # å…¶ä»–ç±»å‹
        return type(data).__name__

    def build_context_snapshot(
        self,
        step_results: Dict[str, Any],
        global_context: Dict[str, Any] = None,
        enriched_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        æ„å»ºå¸¦ Schema æ‘˜è¦çš„ä¸Šä¸‹æ–‡å¿«ç…§

        Args:
            step_results: å„æ­¥éª¤çš„å®Œæ•´æ‰§è¡Œç»“æœ
            global_context: å…¨å±€ä¸Šä¸‹æ–‡
            enriched_context: å¯ŒåŒ–ä¸Šä¸‹æ–‡

        Returns:
            ä¸Šä¸‹æ–‡å¿«ç…§ï¼ŒåŒ…å« _schema å’Œ _ref å­—æ®µ

        Example:
            {
                "global": {
                    "_schema": {"user_id": "string", "tenant_id": "string"},
                    "_data": {"user_id": "123", "tenant_id": "t1"}
                },
                "steps": {
                    "step_1": {
                        "_schema": {"profile": {"name": "string", "age": "int"}},
                        "_ref": "step_results.step_1"
                    }
                }
            }
        """
        snapshot = {}

        # å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆé€šå¸¸è¾ƒå°ï¼Œç›´æ¥åŒ…å«æ•°æ®ï¼‰
        if global_context:
            snapshot["global"] = {
                "_schema": self.build_schema_summary(global_context),
                "_data": global_context  # å…¨å±€ä¸Šä¸‹æ–‡ç›´æ¥åŒ…å«æ•°æ®
            }

        # å¯ŒåŒ–ä¸Šä¸‹æ–‡
        if enriched_context:
            # å¤„ç† ContextEntry ç±»å‹
            processed_enriched = {}
            for k, v in enriched_context.items():
                try:
                    from pydantic import BaseModel
                    if isinstance(v, BaseModel):
                        processed_enriched[k] = v.model_dump()
                    else:
                        processed_enriched[k] = v
                except ImportError:
                    processed_enriched[k] = v

            snapshot["enriched"] = {
                "_schema": self.build_schema_summary(processed_enriched),
                "_data": processed_enriched
            }

        # æ­¥éª¤ç»“æœï¼ˆå¯èƒ½è¾ƒå¤§ï¼Œä½¿ç”¨å¼•ç”¨ï¼‰
        if step_results:
            snapshot["steps"] = {}
            for step_key, step_data in step_results.items():
                snapshot["steps"][step_key] = {
                    "_schema": self.build_schema_summary(step_data),
                    "_ref": f"step_results.{step_key}"
                }

        return snapshot

    def resolve_params_for_tool(
        self,
        tool_schema: Dict[str, Any],
        context_snapshot: Dict[str, Any],
        step_results: Dict[str, Any],
        task_description: str = ""
    ) -> Dict[str, Any]:
        """
        ä¸ºå·¥å…·è°ƒç”¨è§£æå‚æ•°ï¼ˆç»Ÿä¸€å…¥å£ï¼‰

        å·¥ä½œæµç¨‹ï¼š
        1. å°† tool_schema å’Œ context_snapshot._schema äº¤ç»™ LLM
        2. LLM è¿”å›å‚æ•°æ˜ å°„ï¼š{"param_name": "path.to.value"}
        3. æ ¹æ®æ˜ å°„ä»å®é™…æ•°æ®ä¸­æå–å€¼

        Args:
            tool_schema: å·¥å…·çš„å‚æ•°å®šä¹‰ï¼Œæ ¼å¼ï¼š
                {
                    "parameters": {
                        "user_id": {"type": "string", "description": "ç”¨æˆ·ID"},
                        "limit": {"type": "int", "description": "è¿”å›æ•°é‡", "default": 10}
                    },
                    "required": ["user_id"]
                }
            context_snapshot: å¸¦ Schema æ‘˜è¦çš„ä¸Šä¸‹æ–‡å¿«ç…§
            step_results: å®Œæ•´çš„æ­¥éª¤æ‰§è¡Œç»“æœï¼ˆç”¨äºæŒ‰éœ€æå–ï¼‰
            task_description: ä»»åŠ¡æè¿°ï¼ˆå¯é€‰ï¼Œå¸®åŠ© LLM ç†è§£æ„å›¾ï¼‰

        Returns:
            è§£æåçš„å‚æ•°å­—å…¸
        """
        if not self.llm_client:
            self.set_dependencies()

        if not tool_schema or "parameters" not in tool_schema:
            return {}

        # Step 1: æ„å»º LLM Prompt
        param_mapping = self._llm_resolve_param_mapping(
            tool_schema=tool_schema,
            context_snapshot=context_snapshot,
            task_description=task_description
        )

        if not param_mapping:
            return {}

        # Step 2: æ ¹æ®æ˜ å°„æå–å®é™…å€¼
        resolved_params = {}
        for param_name, path in param_mapping.items():
            if path is None or path == "null" or path == "":
                # å‚æ•°æ— æ³•ä»ä¸Šä¸‹æ–‡è·å–ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤å€¼
                param_spec = tool_schema["parameters"].get(param_name, {})
                if "default" in param_spec:
                    resolved_params[param_name] = param_spec["default"]
                continue

            value = self._extract_value_by_path(
                path=path,
                context_snapshot=context_snapshot,
                step_results=step_results
            )

            if value is not None:
                resolved_params[param_name] = value
            else:
                # æå–å¤±è´¥ï¼Œæ£€æŸ¥é»˜è®¤å€¼
                param_spec = tool_schema["parameters"].get(param_name, {})
                if "default" in param_spec:
                    resolved_params[param_name] = param_spec["default"]

        return resolved_params

    def _llm_resolve_param_mapping(
        self,
        tool_schema: Dict[str, Any],
        context_snapshot: Dict[str, Any],
        task_description: str = ""
    ) -> Dict[str, str]:
        """
        ä½¿ç”¨ LLM å†³å®šå‚æ•°æ¥æºæ˜ å°„

        Args:
            tool_schema: å·¥å…·å‚æ•°å®šä¹‰
            context_snapshot: ä¸Šä¸‹æ–‡å¿«ç…§ï¼ˆåªåŒ…å« Schemaï¼‰
            task_description: ä»»åŠ¡æè¿°

        Returns:
            å‚æ•°æ˜ å°„ï¼š{"param_name": "path.to.value"} æˆ– {"param_name": null}
        """
        if not self.llm_client:
            return {}

        # æ„å»ºå‚æ•°è¯´æ˜
        params_info = []
        required_params = tool_schema.get("required", [])

        for param_name, param_spec in tool_schema["parameters"].items():
            param_type = param_spec.get("type", "any")
            param_desc = param_spec.get("description", "")
            is_required = param_name in required_params
            has_default = "default" in param_spec

            req_mark = "[å¿…å¡«]" if is_required else "[å¯é€‰]"
            default_mark = f"(é»˜è®¤: {param_spec['default']})" if has_default else ""

            params_info.append(f"- {param_name}: {param_type} {req_mark} {default_mark}\n  æè¿°: {param_desc}")

        params_text = "\n".join(params_info)

        # æ„å»ºä¸Šä¸‹æ–‡ Schema è¯´æ˜
        schema_parts = []

        if "global" in context_snapshot:
            global_schema = context_snapshot["global"].get("_schema", {})
            schema_parts.append(f"ã€å…¨å±€ä¸Šä¸‹æ–‡ globalã€‘\n{json.dumps(global_schema, ensure_ascii=False, indent=2)}")

        if "enriched" in context_snapshot:
            enriched_schema = context_snapshot["enriched"].get("_schema", {})
            schema_parts.append(f"ã€å¯ŒåŒ–ä¸Šä¸‹æ–‡ enrichedã€‘\n{json.dumps(enriched_schema, ensure_ascii=False, indent=2)}")

        if "steps" in context_snapshot:
            for step_key, step_info in context_snapshot["steps"].items():
                step_schema = step_info.get("_schema", {})
                schema_parts.append(f"ã€æ­¥éª¤ç»“æœ {step_key}ã€‘\n{json.dumps(step_schema, ensure_ascii=False, indent=2)}")

        schema_text = "\n\n".join(schema_parts) if schema_parts else "æ— å¯ç”¨ä¸Šä¸‹æ–‡"

        # æ„å»º Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å‚æ•°æ˜ å°„å™¨ã€‚è¯·æ ¹æ®å·¥å…·æ‰€éœ€å‚æ•°å’Œå¯ç”¨ä¸Šä¸‹æ–‡ï¼Œå†³å®šæ¯ä¸ªå‚æ•°åº”è¯¥ä»å“ªé‡Œè·å–ã€‚

ã€ä»»åŠ¡æè¿°ã€‘
{task_description or "æ‰§è¡Œå·¥å…·è°ƒç”¨"}

ã€å·¥å…·æ‰€éœ€å‚æ•°ã€‘
{params_text}

ã€å¯ç”¨ä¸Šä¸‹æ–‡ç»“æ„ã€‘
{schema_text}

ã€ä»»åŠ¡ã€‘
ä¸ºæ¯ä¸ªå‚æ•°æŒ‡å®šæ•°æ®æ¥æºè·¯å¾„ã€‚è·¯å¾„æ ¼å¼ï¼š
- global.xxx: ä»å…¨å±€ä¸Šä¸‹æ–‡è·å–
- enriched.xxx: ä»å¯ŒåŒ–ä¸Šä¸‹æ–‡è·å–
- steps.step_N.xxx: ä»æ­¥éª¤ç»“æœè·å–
- null: æ— æ³•ä»ä¸Šä¸‹æ–‡è·å–ï¼ˆéœ€è¦ç”¨æˆ·è¾“å…¥æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¾“å‡º JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "å‚æ•°å1": "path.to.value",
    "å‚æ•°å2": "global.user_id",
    "å‚æ•°å3": null
}}

æ³¨æ„ï¼š
1. åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Š
2. è·¯å¾„å¿…é¡»ä¸ä¸Šä¸‹æ–‡ç»“æ„åŒ¹é…
3. å¦‚æœå‚æ•°æœ‰é»˜è®¤å€¼ä¸”ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ï¼Œè¾“å‡º null
4. ä¼˜å…ˆä½¿ç”¨æœ€è¿‘çš„æ•°æ®ï¼ˆsteps > enriched > globalï¼‰
"""

        try:
            response = self.llm_client.generate(
                prompt=prompt,
                parse_json=True,
                max_tokens=500
            )

            if isinstance(response, dict):
                return response
            return {}

        except Exception as e:
            self.logger.error(f"LLM param mapping failed: {e}")
            return {}

    def _extract_value_by_path(
        self,
        path: str,
        context_snapshot: Dict[str, Any],
        step_results: Dict[str, Any]
    ) -> Any:
        """
        æ ¹æ®è·¯å¾„ä»ä¸Šä¸‹æ–‡ä¸­æå–å®é™…å€¼

        Args:
            path: æ•°æ®è·¯å¾„ï¼Œå¦‚ "global.user_id" æˆ– "steps.step_1.profile.name"
            context_snapshot: ä¸Šä¸‹æ–‡å¿«ç…§
            step_results: å®Œæ•´çš„æ­¥éª¤ç»“æœ

        Returns:
            æå–çš„å€¼ï¼Œå¦‚æœè·¯å¾„æ— æ•ˆè¿”å› None
        """
        if not path or path == "null":
            return None

        parts = path.split(".")
        if not parts:
            return None

        root = parts[0]
        remaining_path = parts[1:]

        # ç¡®å®šæ•°æ®æº
        if root == "global":
            data = context_snapshot.get("global", {}).get("_data", {})
        elif root == "enriched":
            data = context_snapshot.get("enriched", {}).get("_data", {})
        elif root == "steps":
            if not remaining_path:
                return None
            step_key = remaining_path[0]
            remaining_path = remaining_path[1:]
            # ä» step_results è·å–å®Œæ•´æ•°æ®
            data = step_results.get(step_key, {})
        else:
            # å°è¯•ç›´æ¥ä» step_results è·å–
            data = step_results.get(root, {})
            # å¦‚æœ root ä¸æ˜¯ step keyï¼Œremaining_path éœ€è¦åŒ…å« root ä¹‹åçš„éƒ¨åˆ†

        # æ²¿è·¯å¾„æå–å€¼
        current = data
        for key in remaining_path:
            if isinstance(current, dict):
                current = current.get(key)
            elif isinstance(current, (list, tuple)):
                try:
                    idx = int(key)
                    current = current[idx] if 0 <= idx < len(current) else None
                except (ValueError, IndexError):
                    current = None
            else:
                current = None

            if current is None:
                break

        return current

    def get_missing_required_params(
        self,
        tool_schema: Dict[str, Any],
        resolved_params: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        è·å–ç¼ºå¤±çš„å¿…å¡«å‚æ•°åˆ—è¡¨

        Args:
            tool_schema: å·¥å…·å‚æ•°å®šä¹‰
            resolved_params: å·²è§£æçš„å‚æ•°

        Returns:
            ç¼ºå¤±å‚æ•°åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« name å’Œ description
        """
        missing = []
        required_params = tool_schema.get("required", [])

        for param_name in required_params:
            if param_name not in resolved_params:
                param_spec = tool_schema["parameters"].get(param_name, {})
                missing.append({
                    "name": param_name,
                    "description": param_spec.get("description", ""),
                    "type": param_spec.get("type", "string")
                })

        return missing

